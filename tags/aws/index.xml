<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS on Vijay Kodam</title>
    <link>https://vijay.eu/tags/aws/</link>
    <description>Recent content in AWS on Vijay Kodam</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 01 Dec 2025 20:39:13 +0200</lastBuildDate>
    
	<atom:link href="https://vijay.eu/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Amazon EKS Capabilities</title>
      <link>https://vijay.eu/posts/eks-capabilities/</link>
      <pubDate>Mon, 01 Dec 2025 20:39:13 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/eks-capabilities/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://dev-to-uploads.s3.amazonaws.com/uploads/articles/a6l05jg2q2jt9i3tg15s.png&#34; alt=&#34;EKS Capabilities&#34;&gt;&lt;/p&gt;
&lt;p&gt;A quick summary typed by my own thumbs and infographic generated by Nano Banana Pro :)&lt;/p&gt;
&lt;p&gt;Last year, AWS offloaded EKS node management and scaling by introducing EKS Auto Mode. Customers did not have to deal with scaling nodes, upgrading nodes etc. This gave the customers more time to deploy and manage their applications.&lt;/p&gt;
&lt;p&gt;This year, EKS has decided to offload managing your ArgoCD, ACK and kro. EKS would manage them for you by running them separately. There are pros and cons for this feature. Pro being there is now one less thing to manage. Since ArgoCD, ACK and kro are not running in your EKS worker nodes, you don&amp;rsquo;t pay for them. AWS takes care of running them, scaling, upgrading them and making sure they are running fine. Con is that EKS charges it as part of EKS Capabilities pricing. Not all features might be supported. Customization is not possible.&lt;/p&gt;
&lt;p&gt;So you need to decide whether this extra cost is worth it or not. It depends on the expertise of the team, how many clusters they are managing and how many applications each ArgoCD is managing.&lt;/p&gt;
&lt;p&gt;From the pricing, number of ArgoCD applications dictates how costly it is going to be, rest all other features and capabilities are relatively cheaper.&lt;/p&gt;
&lt;p&gt;My dear friend Jatin Mehrotra wrote an &lt;a href=&#34;https://dev.to/aws-builders/i-created-s3-buckets-using-argocd-ack-with-eks-capabilities-no-controllers-installed-cm0&#34;&gt;excellent blog&lt;/a&gt; about EKS capabilities and also found a bug.&lt;/p&gt;
&lt;p&gt;Image generated using Nano Banana Pro.&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Honest Take on Kiro, AI IDE from AWS</title>
      <link>https://vijay.eu/posts/my-honest-take-on-kiro/</link>
      <pubDate>Thu, 17 Jul 2025 00:24:38 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/my-honest-take-on-kiro/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/kiro-screenshot.png&#34; alt=&#34;kiro&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Amazon Web Services (AWS) released Kiro, an agentic AI IDE yesterday. Built an app with it today. Here is my honest take of this new AI IDE.&lt;/p&gt;
&lt;p&gt;This is my first experience trying Kiro and spent around 8 hours building a TODO app with Google OAuth2 authentication.&lt;/p&gt;
&lt;h2 id=&#34;what-is-kiro&#34;&gt;What is Kiro?&lt;/h2&gt;
&lt;p&gt;AWS released an AI powered agentic IDE powered by Claude 4.0 Sonnet. Cursor and Windsurf are it&amp;rsquo;s competetors in this area. This area has been very hot with startups cloning VS Code and building AI code editors and several of them are worth couple of billion US dollars. You can learn more about Kiro at &lt;a href=&#34;https://kiro.dev/&#34;&gt;https://kiro.dev/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;vibe-code--spec-code&#34;&gt;Vibe Code &amp;amp; Spec Code&lt;/h2&gt;
&lt;p&gt;Kiro supports vibe-coding as well as spec-based coding. I felt that this is a good way to teach industry best practices for a non-software engineer on how to build a software project from scratch.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/spec-code.png&#34; alt=&#34;spec&#34;&gt;&lt;/p&gt;
&lt;p&gt;From your single line wish in natural language, it creates a big set of requirements, Design Document and properly structured Tasks list which includes not only functional tasks but also non-functional and security related tasks.&lt;/p&gt;
&lt;p&gt;Once it is ready, you could review them, edit them and then run tasks one-by-one sequentially.&lt;/p&gt;
&lt;h2 id=&#34;important-features&#34;&gt;Important Features&lt;/h2&gt;
&lt;p&gt;There are two more concepts/features introduced by Kiro other than spec-driven development:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Agent Hooks and&lt;/li&gt;
&lt;li&gt;Agent Steering.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/hooks.png&#34; alt=&#34;hooks&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;agent-hooks&#34;&gt;Agent Hooks&lt;/h3&gt;
&lt;p&gt;These hooks get called when an event occurs. Currently Agent hooks supports triggering based on file operations like create, save and delete. You then mention what the Agent should do when it is triggered in natural language. No scripting, no coding, just plain English.&lt;/p&gt;
&lt;p&gt;In a very short time, I found Agent hooks to be extremely useful.  I created an agent hook to add files to git and then git commit my changes after every task is successfully completed.&lt;/p&gt;
&lt;p&gt;Here is my Agent hook for git commit on file save:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/git-hook.png&#34; alt=&#34;githooks&#34;&gt;&lt;/p&gt;
&lt;p&gt;Icing on the cake was that the Agent hook actually read all my code changes, summarized and added a single line commit comment which mentions what changes were done. The power of LLMs is on display in this usecase.&lt;/p&gt;
&lt;h3 id=&#34;agent-steering&#34;&gt;Agent Steering&lt;/h3&gt;
&lt;p&gt;When you click the created steering docs, Agent steering creates three docs: product, structure and tech stack. They contain all the info related to your project.&lt;/p&gt;
&lt;p&gt;Here is the steering doc for project for my app:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/steering.png&#34; alt=&#34;steering&#34;&gt;&lt;/p&gt;
&lt;p&gt;You could add your coding best practices, tech stack wish list, other conditions here, which will dictate how the code is generated by LLMs.&lt;/p&gt;
&lt;h2 id=&#34;my-todo-app&#34;&gt;My TODO app&lt;/h2&gt;
&lt;p&gt;I created a TODO app with Google sign-in as authentication method, without any AWS tech involved. Kiro is advertised as an independent AI IDE so I wanted to test that aspect.&lt;/p&gt;
&lt;p&gt;You can find the screenshots of the app in the images below.
&lt;img src=&#34;https://vijay.eu/images/todoapp1.png&#34; alt=&#34;todoapp1&#34;&gt;
&lt;img src=&#34;https://vijay.eu/images/todoapp2.png&#34; alt=&#34;todoapp2&#34;&gt;&lt;/p&gt;
&lt;p&gt;I got a working app with working Google authentication. Tested it with my Gmail credentials and it was working flawlessly.&lt;/p&gt;
&lt;h2 id=&#34;improvements&#34;&gt;Improvements?&lt;/h2&gt;
&lt;p&gt;Considering Kiro is released yesterday, I believe my impressions mentioned below might be resolved sooner or later.&lt;/p&gt;
&lt;p&gt;Anyway, here I go&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;model-choice&#34;&gt;Model Choice&lt;/h3&gt;
&lt;p&gt;Currently Kiro is limited to only Claude 4.0 Sonnet and Claude 3.7 Sonnet. Honestly these two are the industry best for coding.&lt;/p&gt;
&lt;p&gt;However, there might be usecases where fast responses are preferred over best responses. In those case, I wish Nova models from AWS are available as a choice.&lt;/p&gt;
&lt;h3 id=&#34;slowness&#34;&gt;Slowness&lt;/h3&gt;
&lt;p&gt;Few times faced slowness, although it is expected as there will be lot of people trying out Kiro along with Claude 4.0 Sonnet.
Claude 3.7 Sonnet felt bit more faster compared to 4.0 but considering the amount of people trying it out and being Kiro in preview phase, I can totally understand this.&lt;/p&gt;
&lt;h3 id=&#34;agent-hooks-1&#34;&gt;Agent Hooks&lt;/h3&gt;
&lt;p&gt;I might have caught a bug in Agent Hooks implementation or I might be wrong. Anyway, let me spell it out. Feel free to correct me if I understood it wrong.&lt;/p&gt;
&lt;p&gt;While trying out Agent hooks, I have configured an Agent hook to be triggered whenever a file is saved. This worked fine if I run one task at a time. Whenever a file save was done, Agent hook was added to the task queue and waited until the tasks before it are done.&lt;/p&gt;
&lt;p&gt;After running one task at a time, I thought I will click &amp;ldquo;Start Task&amp;rdquo; on all the tasks so that all tasks will be executed sequentially. Here, when the first task was being executed, my git commit Agent hook got triggered by a file save and added it to the queue at the end after all the tasks.&lt;/p&gt;
&lt;p&gt;I wanted the Agent hook to be executed after every task so that each task is saved in a separate git commit. However that did not work as I anticipated. This is my understanding of how Agent hook must be called.&lt;/p&gt;
&lt;p&gt;These are early days of the release of Kiro. Will find out more about this feature going forward.&lt;/p&gt;
&lt;h3 id=&#34;debugging-loop&#34;&gt;Debugging loop&lt;/h3&gt;
&lt;p&gt;Few times, when I asked it to debug and fix an issue, it fell into a circular debugging loop and it was doing the same changes over and over again for more than ten times. I had to cancel the task and ask it to explain what it was doing.&lt;/p&gt;
&lt;p&gt;This might not be an issue with Kiro per se but I think it is an issue with LLM.&lt;/p&gt;
&lt;h2 id=&#34;final-note&#34;&gt;Final note&lt;/h2&gt;
&lt;p&gt;Kiro, Agentic AI IDE from AWS has started on a good note and using Claude 4.0 Sonnet as their model choice might be their genius move.&lt;/p&gt;
&lt;p&gt;AI Coding IDEs have been the first multi-billion dollar usecase coming out of Generative AI. AWS move to introduce Kiro at this moment in time will surely capture sizable userbase considering you pay 19$ per month to get the industry top LLM for coding.&lt;/p&gt;
&lt;p&gt;If you have not tried Kiro, do give it a try. Let me know in comments what is your Kiro experience?&lt;/p&gt;
&lt;h2 id=&#34;follow-me&#34;&gt;Follow Me&lt;/h2&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Built an AI Agent using Strands Agents SDK</title>
      <link>https://vijay.eu/posts/built-an-ai-agent/</link>
      <pubDate>Mon, 07 Jul 2025 23:44:09 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/built-an-ai-agent/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/strands.png&#34; alt=&#34;strands&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Built an AI Agent using Strands Agents SDK from Amazon Web Services (AWS) which calls my Kubernetes MCP server. Read more to find out…&lt;/p&gt;
&lt;h2 id=&#34;agents-and-mcp&#34;&gt;Agents and MCP&lt;/h2&gt;
&lt;p&gt;AI Agents and Model Context Protocol are the most popular concepts in Gen AI now. Now I have created an AI Agent which calls MCP server to debug issues in my K8s cluster.&lt;/p&gt;
&lt;p&gt;Recently I created a Model Context Protocol (MCP) server for &lt;a href=&#34;https://github.com/vijaykodam/kubernetes-readonly-mcp&#34;&gt;Kubernetes read-only operations&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;strands-agents-sdk&#34;&gt;Strands Agents SDK&lt;/h2&gt;
&lt;p&gt;AWS has created an SDK for building AI Agents called &lt;a href=&#34;https://strandsagents.com/latest/&#34;&gt;Strands Agents SDK&lt;/a&gt;. Now it added support for MCP as well.&lt;/p&gt;
&lt;p&gt;I used Strands Agents SDK and built an AI Agent which calls my K8s MCP server and debugs issues in my running K8s cluster.&lt;/p&gt;
&lt;p&gt;This demonstrates the AI Agents ability to achieve goals by perceiving the environment, reasoning and acting upon it using available tools like MCP servers.&lt;/p&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;
&lt;p&gt;Below is my detailed demo of my AI Agent. Links in comments about Strands and my MCP server.&lt;/p&gt;
&lt;iframe 
   src=&#34;https://www.youtube.com/embed/GDEtRssnpx4?si=7S3cvYg-2Bz5PtEa&#34; 
   width=&#34;560&#34; 
   height=&#34;315&#34; 
   title=&#34;Embedded Content&#34; 
   frameborder=&#34;0&#34; 
   allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
   referrerpolicy=&#34;strict-origin-when-cross-origin&#34;
   allowfullscreen&gt;
 &lt;/iframe&gt;

&lt;p&gt;What are you building with Strands Agents SDK?&lt;/p&gt;
&lt;h2 id=&#34;follow-me&#34;&gt;Follow Me&lt;/h2&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Won Amazon Q Coding Challenge</title>
      <link>https://vijay.eu/posts/won-q-coding-challenge/</link>
      <pubDate>Thu, 05 Jun 2025 16:33:12 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/won-q-coding-challenge/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/amazonq-win.jpeg&#34; alt=&#34;amazonqwin&#34;&gt;&lt;/p&gt;
&lt;p&gt;Wow! I won 1st place in the Amazon Q Coding challenge today at AWS Summit Stockholm.&lt;/p&gt;
&lt;p&gt;I had to summon my newfound GenAI vibe coding skills. My experience in working with Amazon Q CLI and Claude Code definitely helped.&lt;/p&gt;
&lt;p&gt;It was a python coding challenge with 40 coding exercises starting with easy level and increasing difficulty with each question. You can use Q developer as a vibe coding tool to solve it. Time also matters and I managed to complete all 40 tasks in 35mins.&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My First Impressions of official EKS MCP Server</title>
      <link>https://vijay.eu/posts/official-eks-mcp-server/</link>
      <pubDate>Fri, 30 May 2025 01:04:32 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/official-eks-mcp-server/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/eksmcp.png&#34; alt=&#34;eksmcp&#34;&gt;
Image Credits: Generated using ChatGPT 3o&lt;/p&gt;
&lt;p&gt;AWS released official EKS MCP Server today!&lt;/p&gt;
&lt;p&gt;I have already created &lt;a href=&#34;https://dev.to/vijaykodam/building-mcp-server-for-kubernetes-4l10&#34;&gt;my own MCP server&lt;/a&gt; for read-only Kubernetes operations which made me appreciate these four things about it.&lt;/p&gt;
&lt;h2 id=&#34;mcp-tools&#34;&gt;MCP Tools&lt;/h2&gt;
&lt;p&gt;Instead of one-to-one mapping of K8s CLI commands to EKS MCP Server tools they have created very few tools by combining functionality. For eg, manage_k8s_resource tool manages all K8s resources and supports create, replace, patch, delete, and read Kubernetes operations.&lt;/p&gt;
&lt;h2 id=&#34;manages-eks-clusters&#34;&gt;Manages EKS Clusters&lt;/h2&gt;
&lt;p&gt;This EKS MCP Server not only can access your EKS clusters but it can also create and manage new EKS clusters. It creates EKS Auto mode clusters by default. It uses CloudFormation to create new EKS Clusters.&lt;/p&gt;
&lt;h2 id=&#34;troubleshooting-support&#34;&gt;Troubleshooting support&lt;/h2&gt;
&lt;p&gt;Really like how they have given access to LLMs their EKS troubleshooting guide via a tool so that LLM can check AWS in-house knowledge of their experience troubleshooting EKS clusters. The tool name is &amp;ldquo;search_eks_troubleshoot_guide&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;secure&#34;&gt;Secure&lt;/h2&gt;
&lt;p&gt;It is always important to have least privileged access. Keeping in with that best practice, EKS MCP Server supports read only mode by default. If you are feeling adventurous add &amp;ldquo;&amp;ndash;allow-write&amp;rdquo; option to give write access. This will still block access to sensitive info. There is another option to give access to that as well.&lt;/p&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;Creating my own kubernetes MCP server gave me valuable insights into how the MCP server creation is done. This EKS MCP server has simplified MCP tooling, added own knowledge of troubleshooting, added least-privileged option and can create EKS clusters using CloudFormation.&lt;/p&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;You can access the official &lt;a href=&#34;https://awslabs.github.io/mcp/servers/eks-mcp-server/&#34;&gt;EKS MCP server here&lt;/a&gt;. You can read about it in &lt;a href=&#34;https://aws.amazon.com/blogs/containers/accelerating-application-development-with-the-amazon-eks-model-context-protocol-server/&#34;&gt;their Blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Really appreciate how EKS MCP server has handled all the functionality really well. Kudos to the EKS team.&lt;/p&gt;
&lt;h2 id=&#34;follow-me&#34;&gt;Follow Me&lt;/h2&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics.&lt;/p&gt;
&lt;p&gt;Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to get into AWS Community Builders Program</title>
      <link>https://vijay.eu/posts/how-to-get-into-aws-community-builders/</link>
      <pubDate>Fri, 23 May 2025 22:24:24 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/how-to-get-into-aws-community-builders/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/VijayCommunityBuilder.webp&#34; alt=&#34;VijayCB&#34;&gt;&lt;/p&gt;
&lt;p&gt;In March 2025, I got into AWS Community Builders Program. Many people reached out to me to know how to get into the program.&lt;/p&gt;
&lt;p&gt;Here is a detailed article on how to get into AWS Community Builders Program with my own experience thrown in. I hope this is useful to you all.&lt;/p&gt;
&lt;h2 id=&#34;what-is-aws-community-builders-program&#34;&gt;What is AWS Community Builders program&lt;/h2&gt;
&lt;p&gt;Here is the official description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The AWS Community Builders program offers technical resources, education, and networking opportunities to AWS technical enthusiasts and emerging thought leaders who are passionate about sharing knowledge and connecting with the technical community.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Official page: &lt;a href=&#34;https://aws.amazon.com/developer/community/community-builders/&#34;&gt;https://aws.amazon.com/developer/community/community-builders/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-do-you-need-to-do&#34;&gt;What do you need to do?&lt;/h2&gt;
&lt;p&gt;Start with the FAQs on the official page. Those have the latest rules and conditions. Read them. Re-read them again. It is important to understand each and everyone of those questions.&lt;/p&gt;
&lt;p&gt;Here are things you need to do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You must have AWS Builder ID. If you don’t have one then create one at &lt;a href=&#34;https://us-east-1.credentials.signin.aws&#34;&gt;https://us-east-1.credentials.signin.aws&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the waitlist for 2026 at &lt;a href=&#34;https://pulse.aws/application/BM2AKLSX?p=0&#34;&gt;https://pulse.aws/application/BM2AKLSX?p=0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The application opens in Jan 2026.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You must have at least two high quality contents created one month before the application opens. This was the condition in 2025, and I guess it will be same in 2026. So make sure you have them ready by Nov 2025.   &lt;/p&gt;
&lt;p&gt;I have purposefully mentioned &lt;strong&gt;two high quality content&lt;/strong&gt; and not an article as there are other mediums as well. You can create Youtube video, write a blog post, talk in local AWS user group, contribute to open source, write a good solution to a question in re:Post. You choose what you want to do. Make sure you have an URL to add in the application which is publicly accessible.&lt;/p&gt;
&lt;p&gt;AWS wants to hear your original voice and original solution. They are interested in how best you can use AWS technology to build an original solution that solves your problem.&lt;/p&gt;
&lt;p&gt;GenAI makes it trivial generating new content, images, and code.
&lt;strong&gt;DON’T USE GenAI/LLM&lt;/strong&gt; generated text or code in your solution. GenAI generated images are okay if you clearly mention image credits immediately after the image. AWS wants to hear your original thoughts, your own experiences, and how you use AWS to improve the solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One Biggest Tip&lt;/strong&gt; I could give you is: Produce content in your mother tongue if English is not your mother tongue. Use it to your own advantage.
AWS encourages you to produce content in your  own local language. You don’t have to use English. Write in Telugu, Finnish, German, Japanese, Turkish, whatever language you know.&lt;/p&gt;
&lt;p&gt;If you are posting on LinkedIn then create content using LinkedIn Articles instead of LinkedIn posts. Posts are temporary. Articles are permanent and are found when you search for topics in or outside of LinkedIn. Also LinkedIn articles supports rich text formatting. Use it to create clear and long content.&lt;/p&gt;
&lt;p&gt;All your contributions must be publicly accessible and not behind any paywalls. Try opening them in a private incognito browser window and test them before adding the URLs in the application. This way you know those URLs are working fine.&lt;/p&gt;
&lt;h2 id=&#34;my-experience&#34;&gt;My Experience&lt;/h2&gt;
&lt;p&gt;In my case, I have been working with AWS for nearly ten years and EKS for the last 4-5 years. I always wanted to contribute back to the community. I have run Linux student tech community back in 2002-2005 and I am a big proponent of open source. There was an itch to write and share my learnings from the start. AWS Community Builders program felt like a correct platform for what I wanted to do in my free time.&lt;/p&gt;
&lt;p&gt;In my case, I had written couple of blogs in 2018 and 2020 and there was a big break. I tried to apply for AWS Community Builders program in 2024 and when I opened the application then I came to know that one needs to add link to their two blog posts or other content written in the last one year and those must be created at least one month before the application. I wished I had known about this before.&lt;/p&gt;
&lt;p&gt;So in mid 2024 I wrote couple of posts on my blog and also started actively posting and writing articles in LinkedIn. When the time came in Jan 2025, I applied and luckily got into the AWS Community Builders program.  This year I have written more blog posts than in the last ten years. I am more motivated to share my learnings, help the community and build solutions in the public.&lt;/p&gt;
&lt;h2 id=&#34;benefits&#34;&gt;Benefits&lt;/h2&gt;
&lt;p&gt;The biggest benefit of joining AWS Community Builders program for me is the access to the network of like-minded builders and AWS experts. Discussing with them when you are stuck on your solutions, learning from what they build, attending builders-only exclusive sessions from AWS experts are the biggest benefits for me.&lt;/p&gt;
&lt;p&gt;Here is the AWS Swag received as part of the program recently.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://dev-to-uploads.s3.amazonaws.com/uploads/articles/55dzooha7vwqau6fceeh.jpeg&#34; alt=&#34;AWS Swag&#34;&gt;&lt;/p&gt;
&lt;p&gt;You get some AWS credits for doing proof of concepts or building different solutions using AWS.&lt;/p&gt;
&lt;p&gt;You also get to post on dedicated organization for builders on dev.to. You get more views and credibility for your posts.&lt;/p&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;First and foremost you need to remember that this is a volunteer role. If you are excited to help others learn AWS technologies, share with the world your AWS learnings, and want to write or talk about it then this program is for you. There will be excellent support from AWS in this regard.&lt;/p&gt;
&lt;p&gt;I personally believe everyone can become an AWS Community Builder if you put the required effort and are willing to help the AWS community.&lt;/p&gt;
&lt;p&gt;This is my way of contributing back to the community by sharing my learnings.&lt;/p&gt;
&lt;h2 id=&#34;follow-me&#34;&gt;Follow Me&lt;/h2&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploying LLMs on Amazon EKS using NVIDIA GPUs</title>
      <link>https://vijay.eu/posts/deploying-llm-on-eks-and-nvidia-gpu/</link>
      <pubDate>Sun, 11 May 2025 19:43:44 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/deploying-llm-on-eks-and-nvidia-gpu/</guid>
      <description>&lt;p&gt;Today I have deployed an LLM inference solution on Amazon EKS using NVidia GPU.&lt;/p&gt;
&lt;p&gt;As part of my Generative AI hands-on learning, attended an AWS hands-on workshop, where I have deployed Mistral 7B Instruct v0.3 model using Ray Serve and vLLM on Amazon EKS.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Below is the architecture diagram of the LLM inference solution I deployed on EKS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/llm-on-eks.jpeg&#34; alt=&#34;llm-on-eks&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;components-used&#34;&gt;Components used&lt;/h2&gt;
&lt;p&gt;If you want to host your own models and control entire lifecycle for security or governance reasons then deploying LLM inference on Amazon EKS is a no-brainer.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/ray-overview/getting-started.html&#34;&gt;Ray&lt;/a&gt; is one of the popular open-source frameworks for building and managing generative AI applications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/serve/index.html&#34;&gt;Ray Serve&lt;/a&gt; is a scalable model serving library for building online inference APIs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.vllm.ai/en/stable/&#34;&gt;vLLM&lt;/a&gt; is a popular high-throughput and memory-efficient inference and serving engine for LLMs. vLLM supports Kubernetes.&lt;/p&gt;
&lt;p&gt;Used &lt;a href=&#34;https://github.com/ray-project/kuberay&#34;&gt;kuberay operator&lt;/a&gt; for deploying Ray. This operator handles all the complexity for you so I prefer this method for deploying Ray on K8s.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/ray-observability/getting-started.html&#34;&gt;Ray dashboard&lt;/a&gt; provides visibility into overall cluster health, jobs, nodes etc.&lt;/p&gt;
&lt;p&gt;Used &lt;a href=&#34;https://github.com/open-webui/open-webui&#34;&gt;Open WebUI&lt;/a&gt; for dashboard. Installed NVIDIA Data Center GPU Manager Exporter for monitoring NVIDIA GPU usage in Grafana.&lt;/p&gt;
&lt;p&gt;Currently, AFAIK, for getting monitoring data from NVIDIA GPUs you have to install the &lt;a href=&#34;https://github.com/NVIDIA/dcgm-exporter&#34;&gt;NVIDIA DCGM exporter&lt;/a&gt;.  It is straight-forward and exports needed metrics like GPU temperature, GPU Power usage, GPU utilization etc.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Ray, Open WebUI, vLLM, Mistral - All are open source software capable of scaling LLM inference very high. This is an exciting development for open source.&lt;/p&gt;
&lt;h2 id=&#34;follow-me&#34;&gt;Follow Me&lt;/h2&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, Generative AI, LLMs, MCP, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generate AWS Arch diagrams using AWS MCP server and Amazon Q CLI</title>
      <link>https://vijay.eu/posts/mcp-amazon-q-cli/</link>
      <pubDate>Sun, 04 May 2025 01:18:22 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/mcp-amazon-q-cli/</guid>
      <description>&lt;p&gt;Recently AWS started adopted Model Context Protocol (MCP) and created first set of AWS MCP servers.&lt;/p&gt;
&lt;p&gt;In this blog post, I will show you how to generate entire AWS architecture diagrams using single prompt with this new AWS MCP server and Amazon Q CLI.&lt;/p&gt;
&lt;p&gt;Here is the generated AWS Architecture diagram:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/aws_data_pipeline.png&#34; alt=&#34;Data pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;Read more to find out how …&lt;/p&gt;
&lt;h2 id=&#34;what-is-mcp&#34;&gt;What is MCP?&lt;/h2&gt;
&lt;p&gt;Model Context Protocol (MCP) is an open protocol that standardizes how applications provide context to LLMs. MCP provides a standardized way to connect AI models to different data sources and tools. You can read more about MCP from &lt;a href=&#34;https://modelcontextprotocol.io/introduction&#34;&gt;their website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;LLMs are essentially text-in-text-out or data-in-data-out systems. Agents or tools give LLMs ability to interact with real world. MCP standardizes the interaction between these agents/tools and the applications, typically via MCP Client on the agent/tool side and MCP server on the application side.&lt;/p&gt;
&lt;p&gt;MCP has become popular after AWS, OpenAI, Google, and Microsoft decided to adopt the standard making it go to protocol. Now we have lots of open source MCP servers ready to use.&lt;/p&gt;
&lt;p&gt;I have tried many MCP Hosts/MCP Clients and Amazon Q CLI is the most simple and straight-forward one. Install Q CLI, login to your AWS Builder ID, and you are good to go on a free tier.
AWS handles the LLM calling transparently. No need of any configurations for LLM.&lt;/p&gt;
&lt;h2 id=&#34;amazon-q-cli&#34;&gt;Amazon Q CLI&lt;/h2&gt;
&lt;p&gt;Amazon Q is a generative AI assistant. Recently Amazon Q CLI announced MCP support. This is a big announcement for me as it simplifies using MCP as simple as calling the Amazon Q CLI. I am a terminal guy and have been using generative AI CLI tools like Claude Code and now Amazon Q CLI.&lt;/p&gt;
&lt;h2 id=&#34;steps-to-install-amazon-q-cli&#34;&gt;Steps to install Amazon Q CLI&lt;/h2&gt;
&lt;p&gt;What I love about Amazon Q CLI is the installation and usage. Installation is a single command in your Mac.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;brew install amazon-q
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You must login using either AWS Builder ID or your AWS credentials. Use AWS Builder ID if you are just getting started. This is an easy way to try it using the free tier.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;q login
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And then you just use it by calling:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;q chat
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Even though we don&amp;rsquo;t interact with LLM directly, Amazon Q CLI uses LLM transparent to the user in the background. Based on the user&amp;rsquo;s query it decides to use the AWS Diagram server via MCP Client inside AWS Q CLI.&lt;/p&gt;
&lt;p&gt;The MCP Client talks to AWS Diagrams MCP server via Model Context Protocol. Internally it pulls the needs icons, generates diagrams using Python diagrams package DSL. Once the image is generated, Q CLI stores it in the user directory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/mcp-q-cli.png&#34; alt=&#34;MCP Amazon Q CLI diagram&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Install GraphViz before. See &lt;a href=&#34;https://www.graphviz.org/&#34;&gt;https://www.graphviz.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In Mac:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;brew install graphviz
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;
&lt;p&gt;In this demo I am setting up &lt;a href=&#34;https://awslabs.github.io/mcp/servers/aws-diagram-mcp-server/&#34;&gt;AWS Diagrams MCP server&lt;/a&gt; which is the tool/MCP server which Amazon Q CLI calls to generate AWS architecture diagram.
Watch below demo where I setup the MCP server and generate AWS architecture diagrams using LLM with a single prompt.&lt;/p&gt;
&lt;iframe 
  src=&#34;https://www.youtube.com/embed/HQK5P_Mbp1g?si=LA5Z9swDLv6T7NgP&#34; 
  width=&#34;560&#34; 
  height=&#34;315&#34; 
  title=&#34;Embedded Content&#34; 
  frameborder=&#34;0&#34; 
  allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
  referrerpolicy=&#34;strict-origin-when-cross-origin&#34;
  allowfullscreen&gt;
&lt;/iframe&gt;

&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Got into AWS Community Builders Program</title>
      <link>https://vijay.eu/posts/aws-community-builders/</link>
      <pubDate>Wed, 05 Mar 2025 10:16:35 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/aws-community-builders/</guid>
      <description>&lt;p&gt;Exciting News!!! I have been accepted into the AWS community Builder Program in the category of Containers!&lt;/p&gt;
&lt;p&gt;Thank you Jason Dunn for the opportunity and looking forward to contributing for the AWS Community Builders program.&lt;/p&gt;
&lt;p&gt;Special Thanks to Rajdeep Saha, AWS Community Nordics, Rolf Koski, Petri Rosenström, Prasad Rao, Ashish Prajapati, Parna Mehta, Aritra Nag, Darryl Ruggles, Darya Petrashka, and Saurabh Shrivastava for inspiring me and helping me in my AWS journey.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/awscommunitybuilder.jpeg&#34; alt=&#34;aws&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pod Disruption Budget vs NodePool Disruption Budget?</title>
      <link>https://vijay.eu/posts/pod-disruption-budget-vs-nodepool-disruption-budget/</link>
      <pubDate>Sun, 02 Mar 2025 23:45:23 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/pod-disruption-budget-vs-nodepool-disruption-budget/</guid>
      <description>&lt;p&gt;In Kubernetes, managing downtimes is very critical to maintain High Availability. &lt;strong&gt;Pod Disruption Budget&lt;/strong&gt; (PDB in short) and &lt;strong&gt;NodePool Disruption Budget&lt;/strong&gt; (NDB in short) plays an important role in managing high availability at different layers of Kubernetes.&lt;/p&gt;
&lt;p&gt;PDBs ensure that a minimum number of application pods are running when one or more nodes are disrupted voluntarily, for example during cluster upgrades, node drain etc.&lt;/p&gt;
&lt;p&gt;You can set pdb value to either &amp;ldquo;&lt;code&gt;minAvailable&lt;/code&gt;&amp;rdquo; or &amp;ldquo;&lt;code&gt;maxUnavailable&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Here are some example usages of PDB:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;By setting &amp;ldquo;&lt;code&gt;minAvailable&lt;/code&gt;&amp;rdquo; to quorum size of 3 when scale is 5 for an etcd cluster, you make sure etcd pods do not reduce below quorum thus keeping the writes from failing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you only run single replica of your application pod and you don&amp;rsquo;t want voluntary disruptions to terminate the application then set &amp;ldquo;&lt;code&gt;maxUnavailable=0&lt;/code&gt;&amp;rdquo;.  In this case, you have to manually evict the pod. This allows you to plan for downtime and then delete the pod manually.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PDBs help prevent downtime by ensuring enough replicas are running when the nodes are down voluntarily.&lt;/p&gt;
&lt;p&gt;Check the popular statefulsets to see how they have configured pdb in their K8s yamls. Here is one such &lt;a href=&#34;https://github.com/strimzi/strimzi-kafka-operator/blob/main/install/cluster-operator/040-Crd-kafka.yaml#L3241&#34;&gt;example&lt;/a&gt; from strimzi-kafka-operator.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NodePool Disruption Budgets&lt;/strong&gt; (NDB) controls how many nodes in a node pool can be disrupted simultaneously, e.g., during rolling updates, Karpenter autoscaling etc.&lt;/p&gt;
&lt;p&gt;NodePools and NDBs are part of Karpenter and you must have Karpenter installed in your Kubernetes cluster before configuring NDBs.&lt;/p&gt;
&lt;p&gt;Karpenter uses NDBs to rate limit Karpenter&amp;rsquo;s disruption. If undefined, it defaults to &amp;ldquo;node:10%&amp;rdquo;. NDBs do not prevent Karpenter from terminating  expired nodes.&lt;/p&gt;
&lt;p&gt;Examples of NDB:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;&lt;code&gt;spec.disruption.budgets.nodes: 10%&lt;/code&gt;&amp;rdquo; will only allow 10% nodes in that nodepool to be disrupted. You can add reasons like &amp;ldquo;if nodes are Empty&amp;rdquo; along with 10% limit.&lt;/li&gt;
&lt;li&gt;Another example for NDB would be to block node disruption first hour during the day for underutilized nodes using:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;     - nodes: &amp;#34;0&amp;#34;
        schedule: &amp;#34;@daily&amp;#34;
        duration: 1h
        reasons:
        - &amp;#34;Underutilized&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can attach multiple budgets to the same NDB.&lt;/p&gt;
&lt;p&gt;Set &amp;ldquo;&lt;code&gt;karpenter.sh/do-not-disrupt: true&lt;/code&gt;&amp;rdquo; to block Karpenter from voluntarily choosing you pod, Deployment, Node or NodePool. Note that setting this annotation does not prevent nodes from forceful disruptions like Expiration, Node Repair etc.&lt;/p&gt;
&lt;p&gt;As you can see in this below diagram, PDB is attached to specific set of pods using a label selector &lt;code&gt;.spec.selector&lt;/code&gt;. You add NDB budget in the NodeClass YAML.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/pdb-ndb-v1.png&#34; alt=&#34;PDB vs NDB&#34;&gt;&lt;/p&gt;
&lt;p&gt;To summarize, use PDB to maintain availability of critical application pods and use NDB to limit disruptions at nodepool level. By combining both, you can increase your pod availability and cluster stability.&lt;/p&gt;
&lt;p&gt;Do you use PDB or NDB? Share your experience in my &lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:7302022325053353984/&#34;&gt;LinkedIn post&lt;/a&gt; comments!&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presented on EKS Auto Mode at the AWS Helsinki Meetup</title>
      <link>https://vijay.eu/posts/presented-at-aws-meetup/</link>
      <pubDate>Thu, 06 Feb 2025 20:17:49 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/presented-at-aws-meetup/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/aws-meetup-1.webp&#34; alt=&#34;AWS Meetup Presenting&#34;&gt;&lt;/p&gt;
&lt;p&gt;Had an amazing time presenting at the AWS meetup today! It was great to share my insights on how EKS Auto mode can help the customers get rid off undifferentiated heavy lifting.&lt;/p&gt;
&lt;p&gt;Started with the basics of Kubernetes for those new to it and then went on to explain how EKS helps customers and how EKS Auto mode will simplify K8s Day 2 operations. The lively Q&amp;amp;A was a highlight, and I learned a lot from the audience&amp;rsquo;s perspectives.  Public speaking is a journey, and I&amp;rsquo;m grateful for the opportunity to practice and grow.&lt;/p&gt;
&lt;p&gt;A huge thank you to Petri Rosenström, Rolf Koski and Accenture for organizing the Amazon Web Services (AWS) meetup today in Helsinki.&lt;/p&gt;
&lt;p&gt;Also learnt about Accenture&amp;rsquo;s technology vision from Juha Takala, Emil Nyback and interesting GenAI talk from Niklas Liljestrand !!! All great talks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/aws-meetup-2.webp&#34; alt=&#34;AWS Meetup Presenting 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>This new EventBridge capability simplifies your cross-account deployments</title>
      <link>https://vijay.eu/posts/eventbridge-new-capability/</link>
      <pubDate>Wed, 22 Jan 2025 12:12:42 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/eventbridge-new-capability/</guid>
      <description>&lt;p&gt;This new capability in &lt;strong&gt;EventBridge&lt;/strong&gt; is going to simplify your cross-account &lt;strong&gt;Event Driven Architecture&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;AWS introduced cross account targets for EventBridge event buses today. Now you can add &lt;strong&gt;SQS&lt;/strong&gt;, &lt;strong&gt;Lambda&lt;/strong&gt; or &lt;strong&gt;SNS&lt;/strong&gt; as targets from a different account. Previously only EventBridge in another account could be added.&lt;/p&gt;
&lt;p&gt;The architecture diagram from the AWS blog is attached in this post. It perfectly captures everything you need to know about this feature.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/eventbridge1.jpg&#34; alt=&#34;AWS Event Bridge&#34;&gt;&lt;/p&gt;
&lt;p&gt;Remember to do these two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add relevant execution &amp;ldquo;role&amp;rdquo; in source AWS account EventBridge rule.&lt;/li&gt;
&lt;li&gt;Apply &amp;ldquo;resource policy&amp;rdquo; to SQS/SNS/Lambda in the Target Account.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Role&amp;rdquo;&lt;/strong&gt; for Source Account and &lt;strong&gt;&amp;ldquo;Resource policy&amp;rdquo;&lt;/strong&gt; for Target account. Everything else is same as before.&lt;/p&gt;
&lt;p&gt;This makes sure security is taken care from both Source and Target AWS accounts and avoids any abuse or DDoS situations.&lt;/p&gt;
&lt;p&gt;Read the AWS Blog &lt;a href=&#34;https://aws.amazon.com/blogs/compute/introducing-cross-account-targets-for-amazon-eventbridge-event-buses/&#34;&gt;post here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Surprising side effect of running EKS Auto Mode</title>
      <link>https://vijay.eu/posts/surprising-side-effect-eks-auto/</link>
      <pubDate>Tue, 21 Jan 2025 12:10:31 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/surprising-side-effect-eks-auto/</guid>
      <description>&lt;p&gt;One surprising side effect you will notice when you move to EKS Auto mode is that you cannot create a Classic Load Balancer (CLB) using Kubernetes Service annotations. You can only create Network Load Balancers (NLB) using K8s Service annotations.&lt;/p&gt;
&lt;p&gt;This restriction came into place due to the automatic inclusion of AWS Load Balancer controller as an add-on in EKS Auto mode. AFAIK, there is no way to disable or remove that add-on from EKS Auto mode cluster.&lt;/p&gt;
&lt;p&gt;If you are utilizing other ingress controllers like HAProxy, then you cannot create a CLB (HTTPS) using Service annotations. You can only create NLBs using Service annotations and route the traffic through HAProxy ingress controller. After that you can create ingresses which can utilize HAProxy as ingress controller.&lt;/p&gt;
&lt;p&gt;This means that if you have been using HAProxy ingress controller or other similar ingress controllers then this limitation restricts you to service HTTP/HTTPS traffic through them.&lt;/p&gt;
&lt;p&gt;Since this is a new release, I haven&amp;rsquo;t seen any update in their docs. I am curious on how other ingress controllers handle this change.&lt;/p&gt;
&lt;p&gt;Have you faced this issue in EKS Auto mode? Did you manage to fix it? I would like to know your thoughts on this?&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Have you seen multi-session support enabled in your AWS Console?</title>
      <link>https://vijay.eu/posts/multi-sessions-in-aws-console/</link>
      <pubDate>Thu, 16 Jan 2025 11:47:06 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/multi-sessions-in-aws-console/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/multisess.png&#34; alt=&#34;multisession&#34;&gt;
Image: Image shows the multi-session support enabled in my AWS Console.&lt;/p&gt;
&lt;p&gt;Observed it today morning that multi-session support is enabled in my AWS account.&lt;/p&gt;
&lt;p&gt;This feature saves lot of effort and time for me. I work with different AWS accounts and logging into those AWS consoles or switching between them is an extra step. Usually I use different browsers to keep both AWS console logins active but it increases CPU/memory load on my PC.&lt;/p&gt;
&lt;p&gt;Now I can use single browser and login to two different accounts simultaneously. Saves CPU/RAM in my PC as I don&amp;rsquo;t have to run two browsers now.&lt;/p&gt;
&lt;p&gt;AWS manages this by adding unique account specific URLs to identify which account you are in even though you access the same EC2 or EKS service in both accounts.&lt;/p&gt;
&lt;p&gt;This is a big feature for me from usability perspective. Currently this is being rolled out and is available only for limited number of user accounts.&lt;/p&gt;
&lt;p&gt;Watch out for this feature in your AWS account and give it a try!!!&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do you run your database on Kubernetes?</title>
      <link>https://vijay.eu/posts/database-on-kubernetes/</link>
      <pubDate>Fri, 10 Jan 2025 11:34:54 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/database-on-kubernetes/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/dbink8s.webp&#34; alt=&#34;dbink8s&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do you run your database on Kubernetes? This has been an evergreen debate. Where does it stand now?&lt;/p&gt;
&lt;p&gt;Kubernetes has come a long way from running only stateless workloads early on to now supporting statefulsets with improvements in sticky identity, persistent storage and unique network identity. Kubernetes has democratized the container orchestration and given the world tools to run most software at scale. This is only one part of the solution.&lt;/p&gt;
&lt;p&gt;Databases built to run directly on servers or virtual machines have different features to make them highly available, resilient and scalable. Now if you bring those databases and run them on Kubernetes then it is a recipe for disaster. In the recent years, there have been cloud native databases built to run on Kubernetes. This is the remaining part of the solution.&lt;/p&gt;
&lt;p&gt;You have a container orchestrator platform like Kubernetes which runs most of your software and now with cloud-native databases, you can run them on Kubernetes too.&lt;/p&gt;
&lt;p&gt;Best of both worlds? What about latency? How do you handle frequent k8s release upgrades every three months? Now with EKS Auto mode, the worker nodes max lifetime is just 21 days. Imagine moving around your database nodes every two-three weeks? Is it recommended in production? Do you instead use managed database service from hashtag#AWS hashtag#GoogleCloud or hashtag#Azure?&lt;/p&gt;
&lt;p&gt;There is no right or wrong answer and &amp;ldquo;It Depends&amp;rdquo; on the software architecture and business constraints.&lt;/p&gt;
&lt;p&gt;I would like to hear from you all. Write your opinion in the comments below. Do you run your database on Kubernetes? Why? Why not? What are your reasons? Are you using it in production? What is the adoption for such databases?&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Image generated using DALL.E to depict storage in the cloud.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is ARC Zonal shift feature in EKS?</title>
      <link>https://vijay.eu/posts/eks-arc-zonal-shift/</link>
      <pubDate>Mon, 30 Dec 2024 01:04:17 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/eks-arc-zonal-shift/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/snowdrop.webp&#34; alt=&#34;snowdrop&#34;&gt;
&lt;em&gt;Image Credits:&lt;/em&gt; Generated using DALL.E. &lt;em&gt;This image depicts &lt;strong&gt;resiliency&lt;/strong&gt; in nature similar to what is expected from your AWS architecture :)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Recently I have created an EKS Auto mode cluster and observed that &lt;strong&gt;“ARC Zonal shift”&lt;/strong&gt; feature was enabled for my EKS cluster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Amazon Application Recovery Controller (ARC)&lt;/strong&gt; helps simplify and automate recovery for highly available applications. It was initially known as “Route 53 ARC” and since then has expanded to support Amazon EC2 Auto scaling groups, ALB, NLB and now EKS. So it is now just Amazon ARC.&lt;/p&gt;
&lt;p&gt;You can recover from an impaired Availability Zone (AZ) using ARC Zonal Shift and Zonal autoshift. Zonal shift is used when you manually trigger it to shift traffic away from an impaired AZ. Use Zonal autoshift to let AWS monitor and shift traffic on your behalf.&lt;/p&gt;
&lt;p&gt;If you want to run resilient and highly available applications across multi-AZs in EKS and you want to survive an AZ going down then this is the feature (EKS Zonal Shift) you need.&lt;/p&gt;
&lt;p&gt;Remember that this will only redirect internal east-west traffic inside your EKS traffic between your pods. If you want to redirect traffic from loadbalancers similarly then you have to enable an ALB or NLB with ARC Zonal shift.&lt;/p&gt;
&lt;p&gt;Performing a zonal shift enables you to achieve rapid recovery from application failures in a single Availability Zone (AZ). This is helpful to build resilience in case of an AZ impairment or when an AZ is down.&lt;/p&gt;
&lt;p&gt;You can enable it from EKS cluster creation step or enable it afterwards for already running clusters. If you are creating EKS auto mode cluster with “Quick configuration” option then ARC Zonal shift is enabled by default.&lt;/p&gt;
&lt;p&gt;With zonal shift, you can temporarily mitigate issues and incidents by triggering a shift and redirecting in-cluster network traffic to a healthy AZ.&lt;/p&gt;
&lt;p&gt;For this to work, you should already be running EKS worker nodes in multiple AZs (at least three) for HA and resiliency and your applications are already running in multiple AZs. So if your application is already running in three different AZs and if one AZ is impaired then Zonal shift will redirect traffic away from impaired AZ to healthy AZs. In this case, you will have your application running in two AZs as highly available after the third one went down.&lt;/p&gt;
&lt;p&gt;Ideally such setup comes with cost so use it for highly critical workloads where you need such level of high-availability and resiliency.&lt;/p&gt;
&lt;p&gt;Check the EKS Zonal shift documentation to learn more: &lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/zone-shift.html&#34;&gt;https://docs.aws.amazon.com/eks/latest/userguide/zone-shift.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Follow me on LinkedIn for more content related to Kubernetes, EKS and AWS in general. Visit my website at &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;https://vijay.eu/posts&lt;/a&gt; for all my posts in one place.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do you know that EKS Auto Mode enforces a 21-day maximum node lifetime?</title>
      <link>https://vijay.eu/posts/eks-auto-max-node-lifetime/</link>
      <pubDate>Thu, 12 Dec 2024 14:13:02 +0530</pubDate>
      
      <guid>https://vijay.eu/posts/eks-auto-max-node-lifetime/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/EKS-Auto-Mode.webp&#34; alt=&#34;Kubernetes&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do you know that EKS Auto Mode enforces a 21-day maximum node lifetime?&lt;/p&gt;
&lt;p&gt;EKS Auto mode cluster manages worker nodes on your behalf.
Karpenter deletes worker nodes in EKS auto mode nodepools after 21 days of node lifetime. This is the maximum node lifetime. It will be replaced with a new node.&lt;/p&gt;
&lt;p&gt;This is needed for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;providing security patches&lt;/li&gt;
&lt;li&gt;OS updates&lt;/li&gt;
&lt;li&gt;component upgrades&lt;/li&gt;
&lt;li&gt;improves security posture&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-if-you-want-to-modify-the-maximum-node-lifetime&#34;&gt;What if you want to modify the maximum node lifetime?&lt;/h2&gt;
&lt;p&gt;You can reduce the maximum node lifetime by creating a custom NodePool with “spec.template.spec.expireAfter” with a value of node lifetime in hours. Max is 21 days.&lt;/p&gt;
&lt;h2 id=&#34;how-to-disable-it&#34;&gt;How to disable it?&lt;/h2&gt;
&lt;p&gt;If for some reason, you do not want to delete worker nodes so frequent or if you want to keep the nodes static then you can create a new Managed Node Group and add nodes in it.
You can create both EKS Auto mode nodepools and Managed Node groups (without auto mode) in the same EKS cluster. These are called mixed-mode clusters.
I will be covering more about mixed-mode clusters in future posts.&lt;/p&gt;
&lt;p&gt;For more insights on EKS, AWS, Kubernetes, and Cloud Architecture, follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and check out &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt; for all my blog posts in one place.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First Impressions of EKS Auto Mode – A Game Changer!</title>
      <link>https://vijay.eu/posts/eks-auto-first-impressions/</link>
      <pubDate>Wed, 04 Dec 2024 23:13:15 +0530</pubDate>
      
      <guid>https://vijay.eu/posts/eks-auto-first-impressions/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/aws-eks-pic.png&#34; alt=&#34;Kubernetes&#34;&gt;
&lt;em&gt;Image Credit: From AWS EKS Blog&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here are my thoughts after initial few days of using &lt;strong&gt;EKS Auto mode&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;AWS has truly raised the bar with EKS Auto Mode, a new feature announced at &lt;strong&gt;re:Invent 2024&lt;/strong&gt;. This innovation automates much of the undifferentiated heavy lifting, allowing customers to focus on what matters most: building great applications.&lt;/p&gt;
&lt;h2 id=&#34;my-hands-on-experience&#34;&gt;My Hands-On Experience&lt;/h2&gt;
&lt;p&gt;I set up an EKS Auto Mode cluster from scratch and was pleasantly surprised by how much AWS now handles post-installation. You can read in &lt;a href=&#34;https://www.linkedin.com/pulse/how-create-eks-auto-mode-cluster-vijay-kumar-kodam-oqw4f/&#34;&gt;more detail about it here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let me break down what &lt;em&gt;Day 2 operations&lt;/em&gt; typically involve:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Managed Node Groups&lt;/strong&gt;: Create managed node groups and wait for worker nodes to come up.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;EKS Node capabilities&lt;/strong&gt;: Most of them were running as Kubernetes Daemonsets on worker nodes before. Now they run as system processes managed by AWS. This includes components such as service discovery, service load balancing, pod networking, block storage, and credential vending.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Add-Ons Installation&lt;/strong&gt;: Select and install versions for multiple EKS add-ons (latest, standard, or older). Like coredns, EBS CSI driver, CNI driver, EKS pod identity etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Load Balancer Controller&lt;/strong&gt;: Install the AWS Load Balancer Controller for ingress traffic handling.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cost Optimization&lt;/strong&gt;: Install Karpenter to manage pod bin-packing, scale nodes dynamically, and select cost-efficient EC2 instances.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OS Security patching&lt;/strong&gt;: Address OS security vulnerabilities identified by AWS Security Hub.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cluster Upgrades&lt;/strong&gt;: Over time, you need to upgrade Kubernetes control plane versions and meticulously plan worker node updates. This is a continuous process every 3-6 months.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, imagine all these tasks being automated by AWS. That’s EKS Auto Mode for you!&lt;/p&gt;
&lt;h2 id=&#34;key-highlights-from-my-testing&#34;&gt;Key Highlights from My Testing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No Idle Nodes&lt;/strong&gt;: When I launched my first EKS Auto Mode cluster, there were no worker nodes running initially—just built-in &amp;ldquo;general-purpose&amp;rdquo; and &amp;ldquo;system&amp;rdquo; node pools.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Scaling&lt;/strong&gt;: After deploying my first application, Karpenter automatically spun up a worker node tailored to the app&amp;rsquo;s resource requirements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: Upon deleting the application, Karpenter scaled down the node, saving money by avoiding idle resources.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This level of automation simplifies cluster operations and significantly reduces costs.&lt;/p&gt;
&lt;h2 id=&#34;pricing-update&#34;&gt;Pricing Update&lt;/h2&gt;
&lt;p&gt;EKS Auto Mode introduces a pay-as-you-go control plane pricing model based on the number of EC2 worker nodes, a shift from the flat fee for the standard EKS control plane. While it’s a slight increase in cost, the time and effort saved make it a Win-Win for both AWS and customers. Find more about &lt;a href=&#34;https://aws.amazon.com/eks/pricing/&#34;&gt;EKS Auto mode pricing here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What’s Next?&lt;/h2&gt;
&lt;p&gt;I’ve &lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:7269311743338663936/&#34;&gt;created a demo video&lt;/a&gt; showcasing EKS Auto Mode in action. If you’re curious, I highly recommend giving this feature a try!&lt;/p&gt;
&lt;p&gt;For more insights on EKS, AWS, Kubernetes, and Cloud Architecture, follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; or check out my blog at &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;https://vijay.eu/posts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let me know your thoughts on this exciting new feature! 👇&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unlock Up to 40% AWS Cost Savings with Graviton!</title>
      <link>https://vijay.eu/posts/save-costs-with-graviton/</link>
      <pubDate>Sun, 27 Oct 2024 00:14:32 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/save-costs-with-graviton/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/graviton.png&#34; alt=&#34;Graviton&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Image Credits: From AWS Blog&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are multiple ways to cost optimize your applications running in AWS. One effective way to achieve this is by using &lt;strong&gt;Graviton-based instance types&lt;/strong&gt;. Graviton processor-based instances are gaining popularity as more applications and AWS services now support them.&lt;/p&gt;
&lt;p&gt;Amazon EC2 instances powered by AWS Graviton2 processors provide up to &lt;strong&gt;40% better price performance&lt;/strong&gt; compared to fifth-generation x86-based instances for a wide variety of workloads.&lt;/p&gt;
&lt;h3 id=&#34;climate-friendly-computing-with-graviton&#34;&gt;Climate-Friendly Computing with Graviton&lt;/h3&gt;
&lt;p&gt;Using Graviton also has climate benefits: it reduces your carbon footprint by using up to &lt;strong&gt;60% less energy&lt;/strong&gt; than comparable EC2 instances for the same performance.&lt;/p&gt;
&lt;p&gt;By choosing Graviton-based instances over traditional x86 instances, you can unlock significant savings. Since Apple adopted ARM processors for their MacBooks, ARM has gained widespread support across operating systems and applications, ensuring seamless compatibility. AWS’s push for Graviton has further motivated the software industry to embrace ARM-based processors.&lt;/p&gt;
&lt;p&gt;AWS has been steadily expanding Graviton support across its services, making it easier than ever to leverage cost savings with ARM-based processors.&lt;/p&gt;
&lt;h3 id=&#34;graviton-support-across-aws-managed-services&#34;&gt;Graviton Support Across AWS Managed Services&lt;/h3&gt;
&lt;p&gt;Here are some of the popular AWS managed services that support Graviton:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Amazon EKS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Amazon ECS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Amazon Aurora&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Amazon RDS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AWS Lambda&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Amazon OpenSearch Service&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For AWS managed services where Graviton-based instances are supported, like EKS, you can create Graviton-based instances for the worker nodes. Refer to this &lt;a href=&#34;https://github.com/aws/aws-graviton-getting-started/blob/main/managed_services.md&#34;&gt;link&lt;/a&gt; for a full list of supported AWS services.&lt;/p&gt;
&lt;h3 id=&#34;graviton-support-in-kubernetes-and-the-container-ecosystem&#34;&gt;Graviton Support in Kubernetes and the Container Ecosystem&lt;/h3&gt;
&lt;p&gt;Kubernetes and containers have become the de facto platform for running microservices today. Graviton is supported by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Amazon EKS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Containerd&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Helm&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grafana&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prometheus&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loki&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Istio&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ArgoCD&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terraform&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;…and many other tools. Here is the &lt;a href=&#34;https://github.com/aws/aws-graviton-getting-started/blob/main/containers.md#ecosystem-support&#34;&gt;full list of popular software&lt;/a&gt; within the container ecosystem that explicitly supports arm64/Graviton.&lt;/p&gt;
&lt;p&gt;Additionally, &lt;strong&gt;.NET 9&lt;/strong&gt; and many older versions support Graviton.&lt;/p&gt;
&lt;h3 id=&#34;operating-systems-supporting-graviton&#34;&gt;Operating Systems Supporting Graviton&lt;/h3&gt;
&lt;p&gt;Several major operating systems also support Graviton, including &lt;strong&gt;Amazon Linux 2023&lt;/strong&gt;, &lt;strong&gt;Ubuntu&lt;/strong&gt;, &lt;strong&gt;RHEL&lt;/strong&gt;, &lt;strong&gt;SUSE&lt;/strong&gt;, &lt;strong&gt;Alpine Linux&lt;/strong&gt;, and more.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;try-amazon-ec2-t4g-instances-for-free-today&#34;&gt;Try Amazon EC2 T4g Instances for Free Today&lt;/h3&gt;
&lt;p&gt;Amazon EC2 T4g instances are the next generation of general-purpose burstable instances powered by Arm-based AWS Graviton2 processors. Try Amazon EC2 &lt;code&gt;t4g.small&lt;/code&gt; instances powered by AWS Graviton2 processors free for up to &lt;strong&gt;750 hours per month until Dec 31st, 2024&lt;/strong&gt;. Refer to the &lt;a href=&#34;https://aws.amazon.com/ec2/faqs/&#34;&gt;Amazon EC2 FAQ&lt;/a&gt; for additional details.&lt;/p&gt;
&lt;p&gt;With this &lt;a href=&#34;https://aws.amazon.com/ec2/graviton/getting-started/&#34;&gt;step-by-step guide from AWS&lt;/a&gt;, you can adopt Graviton-based instances for your workloads.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Consider including Graviton-based instances in your workload, whether in your Kubernetes clusters, EC2 instances, Lambda functions, or databases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Save money and save the planet.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Graviton is one of several ways to optimize costs in AWS. I will cover more cost optimization strategies in future posts, so follow me to learn more about AWS, cost optimization, and Kubernetes. I also post all my articles on &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fastest and Cheapest Ways to Delete Millions of Files from Amazon S3</title>
      <link>https://vijay.eu/posts/s3-delete-millions-of-files/</link>
      <pubDate>Mon, 14 Oct 2024 00:42:32 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/s3-delete-millions-of-files/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Managing data in &lt;strong&gt;Amazon S3&lt;/strong&gt; can be a daunting task, especially when you&amp;rsquo;re faced with the need to delete millions of objects. Whether you’re dealing with old backups, temporary files, or simply restructuring your data, choosing the right method to delete these files is crucial. The right approach can save you time and money, ensuring that you maintain an efficient cloud environment.&lt;/p&gt;
&lt;p&gt;In this post, we’ll explore various methods to delete files from an S3 bucket, highlighting the &lt;strong&gt;fastest&lt;/strong&gt; and &lt;strong&gt;cheapest&lt;/strong&gt; options available.&lt;/p&gt;
&lt;h2 id=&#34;different-ways-to-delete-files-from-s3&#34;&gt;Different Ways to Delete Files from S3&lt;/h2&gt;
&lt;h3 id=&#34;1-aws-management-console&#34;&gt;1. AWS Management Console&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;AWS Management Console&lt;/strong&gt; allows you to manually delete files through a user-friendly interface. While this method is straightforward and suitable for small batches, it becomes impractical for large datasets.&lt;/p&gt;
&lt;h3 id=&#34;2-aws-cli&#34;&gt;2. AWS CLI&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;AWS Command Line Interface (CLI)&lt;/strong&gt; is a popular choice for users who prefer script-based operations. You can use the following command to delete files recursively:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;aws s3 rm s3://bucket-name --recursive
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;However, it’s important to note that the AWS CLI is primarily &lt;strong&gt;single-threaded&lt;/strong&gt;, which can lead to slower performance when deleting large numbers of objects.&lt;/p&gt;
&lt;h3 id=&#34;3-s3cmd&#34;&gt;3. s3cmd&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;s3cmd&lt;/strong&gt; is another command-line tool that offers more features than the AWS CLI. It allows for some level of parallelism, making it &lt;strong&gt;slightly faster&lt;/strong&gt; than the AWS CLI. The command for recursive deletion is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s3cmd del s3://bucket-name --recursive
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;While it provides a better performance boost, it still doesn’t match the speed of more advanced tools.&lt;/p&gt;
&lt;h3 id=&#34;4-s3-batch-operations&#34;&gt;4. S3 Batch Operations&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;S3 Batch Operations&lt;/strong&gt; are designed for large-scale tasks and can delete billions of objects. However, this method incurs additional costs based on the number of objects processed, which may not be ideal for budget-conscious users.&lt;/p&gt;
&lt;h3 id=&#34;5-s3-lifecycle-policies&#34;&gt;5. S3 Lifecycle Policies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;S3 Lifecycle Policies&lt;/strong&gt; are an excellent way to automate the deletion of files without incurring extra costs. You can set rules to automatically delete objects after a specified duration or based on specific conditions. However, deletions may take &lt;strong&gt;up to 24 hours&lt;/strong&gt; to execute.&lt;/p&gt;
&lt;h3 id=&#34;6-s5cmd&#34;&gt;6. s5cmd&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;s5cmd&lt;/strong&gt; is a newer, highly parallelized tool that excels in speed. It can delete thousands of files per second and is particularly useful for massive deletions. To delete files using s5cmd, you can use the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s5cmd rm s3://bucket-name/*
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;comparing-methods-fastest-vs-cheapest&#34;&gt;Comparing Methods: Fastest vs. Cheapest&lt;/h2&gt;
&lt;h3 id=&#34;fastest-method-s5cmd&#34;&gt;Fastest Method: s5cmd&lt;/h3&gt;
&lt;p&gt;When it comes to speed, &lt;strong&gt;s5cmd&lt;/strong&gt; is the clear winner. It is designed for high-performance operations and leverages &lt;strong&gt;multi-threading&lt;/strong&gt; and &lt;strong&gt;batch processing&lt;/strong&gt; to maximize efficiency. With &lt;strong&gt;s5cmd&lt;/strong&gt;, you can expect deletion rates to be up to &lt;strong&gt;100x faster&lt;/strong&gt; than the AWS CLI, making it an excellent choice for scenarios where time is critical.&lt;/p&gt;
&lt;h3 id=&#34;cheapest-method-s3-lifecycle-policies&#34;&gt;Cheapest Method: S3 Lifecycle Policies&lt;/h3&gt;
&lt;p&gt;If minimizing costs is your primary concern, &lt;strong&gt;S3 Lifecycle Policies&lt;/strong&gt; are the way to go. This method allows you to automate deletions without incurring additional charges. You can set lifecycle rules that trigger deletions based on file age or other criteria, making it ideal for long-term data management. While it may take longer to process (up to 24 hours), it eliminates the need for any new costs, making it perfect for &lt;strong&gt;cost-conscious&lt;/strong&gt; environments.&lt;/p&gt;
&lt;h2 id=&#34;why-these-methods-are-the-best&#34;&gt;Why These Methods Are the Best&lt;/h2&gt;
&lt;h3 id=&#34;speed-considerations&#34;&gt;Speed Considerations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;For Speed&lt;/strong&gt;: When you need to delete millions of objects quickly, &lt;strong&gt;s5cmd&lt;/strong&gt; stands out due to its ability to handle multiple requests simultaneously. This is particularly advantageous in environments where data is frequently updated or removed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cost-considerations&#34;&gt;Cost Considerations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;For Cost&lt;/strong&gt;: &lt;strong&gt;S3 Lifecycle Policies&lt;/strong&gt; allow you to automate data management tasks without incurring any additional charges. This is crucial for businesses looking to optimize their cloud costs while maintaining a clean and organized data structure.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Choosing the right method to delete millions of files from an S3 bucket depends on your specific use case. If speed is your priority, &lt;strong&gt;s5cmd&lt;/strong&gt; is the best tool for the job. Conversely, if you’re focused on minimizing costs, &lt;strong&gt;S3 Lifecycle Policies&lt;/strong&gt; offer an automated, no-cost solution for managing your data over time.&lt;/p&gt;
&lt;p&gt;By understanding these options, you can make informed decisions that streamline your data management processes in AWS S3, saving both time and money in the long run.&lt;/p&gt;
&lt;p&gt;Follow me on LinkedIn at &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;https://www.linkedin.com/in/vijaykodam/&lt;/a&gt;  where I post articles about AWS, Kubernetes and cloud computing in general.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Save Hours During Amazon EKS Upgrades with This One Simple Step!</title>
      <link>https://vijay.eu/posts/eks-upgrade-insights/</link>
      <pubDate>Thu, 03 Oct 2024 01:00:19 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/eks-upgrade-insights/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/K8s.png&#34; alt=&#34;Kubernetes&#34;&gt;&lt;/p&gt;
&lt;p&gt;🚀 Save Hours During Amazon EKS Upgrades with This One Simple Step! 🚀&lt;/p&gt;
&lt;p&gt;If you’ve ever managed EKS clusters in production, you know how crucial it is to keep them updated to leverage the latest features and maintain security.&lt;/p&gt;
&lt;p&gt;🔄 But before starting an upgrade, have you checked the &amp;ldquo;Upgrade Insights&amp;rdquo; tab on the EKS cluster page?&lt;/p&gt;
&lt;p&gt;Previously, our team had to manually check for deprecations and potential issues before every upgrade, spending hours in the process. Now, with the &amp;ldquo;Upgrade Insights&amp;rdquo; feature, EKS continuously monitors audit logs, detects deprecation errors, and updates the insights daily.&lt;/p&gt;
&lt;p&gt;This small step has transformed our process and saved us countless hours of debugging and maintenance. 💡&lt;/p&gt;
&lt;p&gt;✅ &lt;strong&gt;Pro tip&lt;/strong&gt;: Always review the &amp;ldquo;Upgrade Insights&amp;rdquo; tab before upgrading your clusters to ensure a smooth transition to newer Kubernetes versions.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Send your feedback by commenting on my LinkedIn post below.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/posts/activity-7247354293660356609-IT7t&#34;&gt;https://www.linkedin.com/posts/activity-7247354293660356609-IT7t&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Where Should I Deploy My K8s Cluster?</title>
      <link>https://vijay.eu/posts/deploy-k8s/</link>
      <pubDate>Sun, 17 May 2020 01:00:19 +0300</pubDate>
      
      <guid>https://vijay.eu/posts/deploy-k8s/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/K8s.png&#34; alt=&#34;Kubernetes&#34;&gt;&lt;/p&gt;
&lt;p&gt;This week there was an &lt;a href=&#34;https://www.redhat.com/en/blog/red-hat-and-aws-extend-collaboration-introducing-amazon-red-hat-openshift&#34;&gt;announcement&lt;/a&gt; about Amazon Red Hat Openshift. It is an enterprise Kubernetes (K8s) service on AWS jointly managed and supported by AWS and Red Hat. Upon reading more about the service, found out that Red Hat already has two more OpenShift services available on AWS. If you count AWS&amp;rsquo; own managed K8s service Amazon Elastic Kubernetes Service (EKS) then there are four different ways you can run a K8s cluster on top of AWS. I am sure there are many other companies providing similar managed K8s services on top of AWS.&lt;/p&gt;
&lt;p&gt;For a beginner starting to use K8s this is overwhelming. This brings us to the question: &lt;strong&gt;Where should I deploy my K8s cluster?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As always, the answer is: &lt;strong&gt;It Depends&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It depends&lt;/strong&gt; on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your team&amp;rsquo;s K8s expertise&lt;/li&gt;
&lt;li&gt;Your company&amp;rsquo;s budget&lt;/li&gt;
&lt;li&gt;Your data locality requirements&lt;/li&gt;
&lt;li&gt;Your preferred Cloud Vendor&lt;/li&gt;
&lt;li&gt;Your company&amp;rsquo;s already existing deals with Software vendors&lt;/li&gt;
&lt;li&gt;and many more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You could run your K8s cluster in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On Premises&lt;/li&gt;
&lt;li&gt;Hybrid Cloud&lt;/li&gt;
&lt;li&gt;IaaS&lt;/li&gt;
&lt;li&gt;PaaS&lt;/li&gt;
&lt;li&gt;Others&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;on-premises&#34;&gt;On Premises&lt;/h3&gt;
&lt;p&gt;If you already own datacenters or if you have strict privacy/security requirements for making sure the data does not leave your premises then &lt;em&gt;On Premises&lt;/em&gt; solution is the way to go.&lt;/p&gt;
&lt;p&gt;Install host operating system on the bare metal servers and then install K8s on top of it. Kubeadm is one option. However, be aware that this is a bare bones solution. You have to build/integrate authentication, authorization, dashboard, security, networking plugins, service mesh, storage, the list goes on.&lt;/p&gt;
&lt;p&gt;You could choose to install Openshift or Rancher. These come fully loaded.&lt;/p&gt;
&lt;p&gt;On Premises solutions are usually the slowest ones  to complete the installation as you have to deal  with hardware. It takes time to order, ship, install and configure them.&lt;/p&gt;
&lt;h3 id=&#34;hybrid-cloud&#34;&gt;Hybrid Cloud&lt;/h3&gt;
&lt;p&gt;Amazon Outposts, Google Anthos and Azure Stack provide rack full of servers which you can install in your datacenter. These racks are connected to their Public cloud and you manage it just like VMs on public cloud.&lt;/p&gt;
&lt;p&gt;This option gives you the flexibility of cloud deployment with the advantage of not managing the hardware.&lt;/p&gt;
&lt;p&gt;Keep in mind that this is the most costliest option. Bill can run into millions easily.&lt;/p&gt;
&lt;p&gt;Once you have Outposts, Anthos or Azure Stack rack on premises, you can use their managed K8s solution on top of it. Google Anthos GKE is one such option.&lt;/p&gt;
&lt;p&gt;The timeline depends on the cloud provider and honestly I have no idea about it.&lt;/p&gt;
&lt;h3 id=&#34;iaas&#34;&gt;IaaS&lt;/h3&gt;
&lt;p&gt;If you need full control of the K8s cluster and you are a pro in managing K8s then this is the option to go.&lt;/p&gt;
&lt;p&gt;You install K8s on top of Amazon EC2 or Google Compute Engine or Azure Virtual Machine.&lt;/p&gt;
&lt;p&gt;Several K8s deployment tools like kops, kubespray or KRIB exist. You can also install Red Hat Openshift or Rancher on the virtual machines.&lt;/p&gt;
&lt;p&gt;Use this option only when you have experience running k8s clusters.&lt;/p&gt;
&lt;h3 id=&#34;paas&#34;&gt;PaaS&lt;/h3&gt;
&lt;p&gt;If all you wanted is a K8s cluster and don&amp;rsquo;t know or don&amp;rsquo;t want to know K8s cluster management then this option is for you.&lt;/p&gt;
&lt;p&gt;Managed K8s solutions like Google GKE, Amazon EKS, Amazon Red Hat Openshift, Azure AKS does fit the bill.&lt;/p&gt;
&lt;p&gt;You click a button and you get a cluster and the kubeconfig/credentials to the cluster.&lt;/p&gt;
&lt;p&gt;You might want to customize some options, enable logging, move the API server to private endpoint etc.&lt;/p&gt;
&lt;p&gt;Usually this is a good place to start for development clusters. Deploy the k8s cluster, tune it, test it, run your applications and then customize more.&lt;/p&gt;
&lt;p&gt;Since this is a managed solution, you will not have full control of the cluster. You have to use whatever version they support, don&amp;rsquo;t get access to the API server or etcd servers barring some flags.&lt;/p&gt;
&lt;h3 id=&#34;others&#34;&gt;Others&lt;/h3&gt;
&lt;p&gt;Minikube, kind, k3s are for developments purposes. These software are light weight and are designed to run on your laptop.&lt;/p&gt;
&lt;p&gt;These solutions can be used for learning about k8s, for local testing of your applications.&lt;/p&gt;
&lt;p&gt;K8s distributions like Red Hat Openshift or Rancher can be installed on bare metal, IaaS, and  PaaS. Usually this option is useful if you have more than one type of infrastructure and you want to use the same K8s distribution everywhere. You could build automation on top of it and deploy it any where you want.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post I have tried to cover different ways you can deploy your kubernetes cluster. This list is not exhaustive and I might have missed some options.&lt;/p&gt;
&lt;p&gt;Kubernetes has become the industry standard for running containers and all the major public cloud providers have K8s services.&lt;/p&gt;
&lt;p&gt;Purpose of writing this blog post is to showcase the variety of K8s deployment options you have, be it on a bare metal server, or virtual machine or managed solution like this week&amp;rsquo;s announcement of Amazon Red Hat Openshift.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Send your feedback by commenting on my tweet below.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Where should I deploy my K8s cluster?&lt;br&gt;My new blog post on this topic.&lt;a href=&#34;https://t.co/sflPzY3Rnt&#34;&gt;https://t.co/sflPzY3Rnt&lt;/a&gt;&lt;br&gt;&lt;br&gt;Do read it and give me feedback.&lt;/p&gt;&amp;mdash; Vijay Kodam (@vijaykodam) &lt;a href=&#34;https://twitter.com/vijaykodam/status/1261783876596350976?ref_src=twsrc%5Etfw&#34;&gt;May 16, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


</description>
    </item>
    
  </channel>
</rss>
