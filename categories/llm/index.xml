<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on Vijay Kodam</title>
    <link>https://vijay.eu/categories/llm/</link>
    <description>Recent content in LLM on Vijay Kodam</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 24 Feb 2025 23:58:17 +0200</lastBuildDate>
    
	<atom:link href="https://vijay.eu/categories/llm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Claude Code Agentic CLI demo</title>
      <link>https://vijay.eu/posts/claude-code-demo/</link>
      <pubDate>Mon, 24 Feb 2025 23:58:17 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/claude-code-demo/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/claude-code-shell.png&#34; alt=&#34;claude-code&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here is my Day Zero hands-on demo of Claude Code using Claude 3.7 Sonnet. Both were released by Anthropic today.&lt;/p&gt;
&lt;p&gt;Claude 3.7 Sonnet, is the first hybrid reasoning model on the market by Anthropic. They have also introduced a command line tool for agentic coding, Claude Code.&lt;/p&gt;
&lt;p&gt;I have installed in my mac and tried agentic code and made a demo video for you.&lt;/p&gt;
&lt;iframe 
  src=&#34;https://www.youtube.com/embed/mFN-MGEVMMI?si=a_ZpbzvN6GBuacZu&#34; 
  width=&#34;560&#34; 
  height=&#34;315&#34; 
  title=&#34;Embedded Content&#34; 
  frameborder=&#34;0&#34; 
  allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
  referrerpolicy=&#34;strict-origin-when-cross-origin&#34;
  allowfullscreen&gt;
&lt;/iframe&gt;

&lt;p&gt;Take a look and add in comments how are you planning to use this shiny new agentic code tool in your workflow?&lt;/p&gt;
&lt;p&gt;It cost me $0.3273 for 1 minute usage of Anthropic API. It is costly for personal use. That is my personal feeling.
&lt;img src=&#34;https://vijay.eu/images/claude-code-cost.png&#34; alt=&#34;claude-code-cost&#34;&gt;&lt;/p&gt;
&lt;p&gt;I would rather want this Claude code like agentic code tool to run locally and call a local llm running on my mac. Everything local, and secure solution. Of course, there will be some opensource tool out in the market in a month doing the same thing. Let&amp;rsquo;s hope so&amp;hellip;&lt;/p&gt;
&lt;p&gt;What do you think? Write in comments.&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classic Snake Game using LLMs</title>
      <link>https://vijay.eu/posts/snake-game/</link>
      <pubDate>Sun, 16 Feb 2025 23:23:47 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/snake-game/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/SnakeRedApple.jpeg&#34; alt=&#34;snake&#34;&gt;
Image Credit: Generated using Google Imagen3.&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;Last August, I have spent a week trying to create working applications using GenAI models. These were meant to be useful for me and my kids and to assess the capabilities of the then popular LLMs like ChatGPT, Claude, and Gemini. You can find them here &lt;a href=&#34;https://vijay.eu/projects/&#34;&gt;https://vijay.eu/projects/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;my-initial-experience&#34;&gt;My initial experience&lt;/h2&gt;
&lt;p&gt;This was my experience back then: Ask each LLM to generate code for a project, copy the code into local files, test it locally, come up with new suggestions and then rinse and repeat until I found the project working well as per my expectation.&lt;/p&gt;
&lt;p&gt;For this exercise I have used: Anthropic Claude, Google Gemini, and OpenAI ChatGPT.
Each had a different way of generating code, handling code changes on top of existing code and updating code.&lt;/p&gt;
&lt;p&gt;From my experience at that time, Anthropic Claude understood prompts really well and generated code much better than all other LLMs. The quality of code was way better than others. One major advantage was Claude Artifacts. This was a deal breaker for me and game changer for testing quick prototypes in the chat window itself. Also Claude allowed you to host the code and gave you an URL to share with your friends. Getting the html files and testing locally also worked seemlessly. It let you download the files. No copy pasting code from the chat window, which was the case for the remaining two LLMs.&lt;/p&gt;
&lt;h2 id=&#34;new-challenge&#34;&gt;New Challenge&lt;/h2&gt;
&lt;p&gt;Then came the other challenge. My website(&lt;a href=&#34;https://vijay.eu&#34;&gt;https://vijay.eu&lt;/a&gt;) is based on Hugo and it is a static website hosted on GitHub Pages. GitHub delivers the static webpages to your mobile/computer when you open my website. There is no dynamic content in my website so there is no need of backend server to process the requests.&lt;/p&gt;
&lt;p&gt;I wanted to host my projects created using LLMs also on my website. This means, those projects should be fully static and should not require a backend server to handle any requests for the frontend running in user&amp;rsquo;s mobile or computer.&lt;/p&gt;
&lt;p&gt;Hugo uses templating which simplifies adding new posts, which means all posts have the same header, footer, common javascript and common CSS. However for my projects, most of the magic is handled as part of Javascript and CSS.&lt;/p&gt;
&lt;p&gt;I spent several hours tweaking the code, explaining this to LLMs and asking them to generate the code which works with my existing hugo code. This took quite an effort and took several trial and errors before the first project worked. When I wanted to create a second flag game I had to do all this over again as it included a different design and there were upto ten questions each coming one after another. I enjoyed building those games but thought these should have been handled automatically for me.&lt;/p&gt;
&lt;h2 id=&#34;current-experience-with-llms&#34;&gt;Current experience with LLMs&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Today after four months&lt;/strong&gt;, I wanted to check how far these LLMs have improved. So I decided to create classic snake game for my kids to play. Asked the same prompt to Claude, Gemini and ChatGPT. ChatGPT has new feature similar to Claude&amp;rsquo;s artifact. ChatGPT created the code and displayed the code and demo window. When I pressed Preview, it kinda worked until the green snake (a single green square) ate the first red square. After that it all went haywire. There were multiple red squares appearing randomly across the box and the green square does not move.&lt;/p&gt;
&lt;p&gt;Gemini generated code in a window and no option to run locally in the window. Copied the code to my computer and ran it locally. It didn&amp;rsquo;t work. There was a black box and nothing else. No red or green squares.&lt;/p&gt;
&lt;p&gt;Now, Claude generated perfectly working code, tested it in the artifact window and also customized the code to work with my hugo website. It also gave me instructions to add shortcodes in my hugo to add new javascript and css for this game. I felt the snake was moving too fast and asked Claude to generate new version to add three different speeds. It did on first try. I have tested it locally then added it my blog and pushed the changes. You can try this snake game on my website at &lt;a href=&#34;https://vijay.eu/projects/snake-game/&#34;&gt;https://vijay.eu/projects/snake-game/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;winner-this-time&#34;&gt;Winner this time&lt;/h3&gt;
&lt;p&gt;So, in this second round of my game benchmarking of LLMs, the winner is &lt;strong&gt;Anthropic Claude&lt;/strong&gt; :)&lt;/p&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;These LLMs are now getting more advanced, producing error free code and understanding user prompts much better. This took me 10 minutes to prompt, test, download and push it to my website. TEN minutes from getting the idea to publishing it live on my website. This is A-W-E-S-O-M-E. You just have to imagine and you have unlimited digital workers, a.k.a LLMs, realizing those dreams for you. Democratizing code for everyone. This gives more power to people who can create end-to-end projects from idea to design to code to deploying it in real world production.&lt;/p&gt;
&lt;p&gt;How was your experience working with LLMs? What difference have you observed over the last six months?&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running Deepseek R1 locally</title>
      <link>https://vijay.eu/posts/running-deepseek-locally/</link>
      <pubDate>Wed, 29 Jan 2025 22:12:06 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/running-deepseek-locally/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/snowdrop.png&#34; alt=&#34;snowdrop&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here is a video of Deepseek R1 running locally on my Macbook.
Now that everyone is amazed by the low level of resource utilization and open weight model of Deepseek R1, installed 8B model locally.&lt;/p&gt;
&lt;p&gt;With the help of ollama it is as simple as running a single command -
&amp;ldquo;ollama run deepseek-r1:8b&amp;rdquo;
You get your own private, local LLM running securely in your computer.&lt;/p&gt;
&lt;p&gt;My prompt is &amp;ldquo;Generate a five line fairy tale about AI?&amp;rdquo;. In the video, you can see how before generating the response, it thinks. Thinking text is in between &lt;!-- raw HTML omitted --&gt; and &lt;!-- raw HTML omitted --&gt; tags.&lt;/p&gt;
&lt;p&gt;Watch the video to read the five line fairy tale about AI.&lt;/p&gt;
&lt;p&gt;Do you have any experience running Deepseek R1?&lt;/p&gt;
&lt;iframe 
  src=&#34;https://www.youtube.com/embed/CYfpgvsTG9E?si=QKfJcJB1vjJIRWwe&#34; 
  width=&#34;560&#34; 
  height=&#34;315&#34; 
  title=&#34;Embedded Content&#34; 
  frameborder=&#34;0&#34; 
  allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
  referrerpolicy=&#34;strict-origin-when-cross-origin&#34;
  allowfullscreen&gt;
&lt;/iframe&gt;

&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New metric to measure the efficiency of AI models</title>
      <link>https://vijay.eu/posts/new-llm-metric/</link>
      <pubDate>Tue, 28 Jan 2025 20:03:20 +0200</pubDate>
      
      <guid>https://vijay.eu/posts/new-llm-metric/</guid>
      <description>&lt;p&gt;At the end of the day, how cheaply and energy efficiently can you generate tokens matters.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;&lt;strong&gt;Tokens per watt per dollar&lt;/strong&gt;&amp;rdquo; metric mentioned by Satya Nadella, CEO of Microsoft, is trying to achieve the same.&lt;/p&gt;
&lt;p&gt;This will be a metric by which every LLM is going to be benchmarked against. This is the metric which will showcase how long can an AI investment lasts and how much is your profitability of AI startups.&lt;/p&gt;
&lt;p&gt;There will be a comparison of LLMs with &amp;ldquo;Tokens per watt per dollar&amp;rdquo; soon, if not already existing.&lt;/p&gt;
&lt;p&gt;I would love to hear your thoughts on this?&lt;/p&gt;
&lt;p&gt;This is &lt;a href=&#34;https://www.linkedin.com/posts/satyanadella_wef25-activity-7287900710770196480-7mn6/&#34;&gt;related post&lt;/a&gt; by Satya.&lt;/p&gt;
&lt;p&gt;If you have not already watched, highly recommend Satya Nadella&amp;rsquo;s &lt;a href=&#34;https://www.youtube.com/watch?v=kOkDTvsUuWA&amp;amp;t=156s&#34;&gt;AI Tour Keynote: London&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
