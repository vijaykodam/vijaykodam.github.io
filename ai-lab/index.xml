<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Lab on Vijay Kodam</title>
    <link>https://vijay.eu/ai-lab/</link>
    <description>Recent content in AI Lab on Vijay Kodam</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 17 Jul 2025 00:24:38 +0300</lastBuildDate>
    
	<atom:link href="https://vijay.eu/ai-lab/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>My Honest Take on Kiro, AI IDE from AWS</title>
      <link>https://vijay.eu/ai-lab/my-honest-take-on-kiro/</link>
      <pubDate>Thu, 17 Jul 2025 00:24:38 +0300</pubDate>
      
      <guid>https://vijay.eu/ai-lab/my-honest-take-on-kiro/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/kiro-screenshot.png&#34; alt=&#34;kiro&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Amazon Web Services (AWS) released Kiro, an agentic AI IDE yesterday. Built an app with it today. Here is my honest take of this new AI IDE.&lt;/p&gt;
&lt;p&gt;This is my first experience trying Kiro and spent around 8 hours building a TODO app with Google OAuth2 authentication.&lt;/p&gt;
&lt;h2 id=&#34;what-is-kiro&#34;&gt;What is Kiro?&lt;/h2&gt;
&lt;p&gt;AWS released an AI powered agentic IDE powered by Claude 4.0 Sonnet. Cursor and Windsurf are it&amp;rsquo;s competetors in this area. This area has been very hot with startups cloning VS Code and building AI code editors and several of them are worth couple of billion US dollars. You can learn more about Kiro at &lt;a href=&#34;https://kiro.dev/&#34;&gt;https://kiro.dev/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;vibe-code--spec-code&#34;&gt;Vibe Code &amp;amp; Spec Code&lt;/h2&gt;
&lt;p&gt;Kiro supports vibe-coding as well as spec-based coding. I felt that this is a good way to teach industry best practices for a non-software engineer on how to build a software project from scratch.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/spec-code.png&#34; alt=&#34;spec&#34;&gt;&lt;/p&gt;
&lt;p&gt;From your single line wish in natural language, it creates a big set of requirements, Design Document and properly structured Tasks list which includes not only functional tasks but also non-functional and security related tasks.&lt;/p&gt;
&lt;p&gt;Once it is ready, you could review them, edit them and then run tasks one-by-one sequentially.&lt;/p&gt;
&lt;h2 id=&#34;important-features&#34;&gt;Important Features&lt;/h2&gt;
&lt;p&gt;There are two more concepts/features introduced by Kiro other than spec-driven development:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Agent Hooks and&lt;/li&gt;
&lt;li&gt;Agent Steering.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/hooks.png&#34; alt=&#34;hooks&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;agent-hooks&#34;&gt;Agent Hooks&lt;/h3&gt;
&lt;p&gt;These hooks get called when an event occurs. Currently Agent hooks supports triggering based on file operations like create, save and delete. You then mention what the Agent should do when it is triggered in natural language. No scripting, no coding, just plain English.&lt;/p&gt;
&lt;p&gt;In a very short time, I found Agent hooks to be extremely useful.  I created an agent hook to add files to git and then git commit my changes after every task is successfully completed.&lt;/p&gt;
&lt;p&gt;Here is my Agent hook for git commit on file save:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/git-hook.png&#34; alt=&#34;githooks&#34;&gt;&lt;/p&gt;
&lt;p&gt;Icing on the cake was that the Agent hook actually read all my code changes, summarized and added a single line commit comment which mentions what changes were done. The power of LLMs is on display in this usecase.&lt;/p&gt;
&lt;h3 id=&#34;agent-steering&#34;&gt;Agent Steering&lt;/h3&gt;
&lt;p&gt;When you click the created steering docs, Agent steering creates three docs: product, structure and tech stack. They contain all the info related to your project.&lt;/p&gt;
&lt;p&gt;Here is the steering doc for project for my app:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/steering.png&#34; alt=&#34;steering&#34;&gt;&lt;/p&gt;
&lt;p&gt;You could add your coding best practices, tech stack wish list, other conditions here, which will dictate how the code is generated by LLMs.&lt;/p&gt;
&lt;h2 id=&#34;my-todo-app&#34;&gt;My TODO app&lt;/h2&gt;
&lt;p&gt;I created a TODO app with Google sign-in as authentication method, without any AWS tech involved. Kiro is advertised as an independent AI IDE so I wanted to test that aspect.&lt;/p&gt;
&lt;p&gt;You can find the screenshots of the app in the images below.
&lt;img src=&#34;https://vijay.eu/images/todoapp1.png&#34; alt=&#34;todoapp1&#34;&gt;
&lt;img src=&#34;https://vijay.eu/images/todoapp2.png&#34; alt=&#34;todoapp2&#34;&gt;&lt;/p&gt;
&lt;p&gt;I got a working app with working Google authentication. Tested it with my Gmail credentials and it was working flawlessly.&lt;/p&gt;
&lt;h2 id=&#34;improvements&#34;&gt;Improvements?&lt;/h2&gt;
&lt;p&gt;Considering Kiro is released yesterday, I believe my impressions mentioned below might be resolved sooner or later.&lt;/p&gt;
&lt;p&gt;Anyway, here I go&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;model-choice&#34;&gt;Model Choice&lt;/h3&gt;
&lt;p&gt;Currently Kiro is limited to only Claude 4.0 Sonnet and Claude 3.7 Sonnet. Honestly these two are the industry best for coding.&lt;/p&gt;
&lt;p&gt;However, there might be usecases where fast responses are preferred over best responses. In those case, I wish Nova models from AWS are available as a choice.&lt;/p&gt;
&lt;h3 id=&#34;slowness&#34;&gt;Slowness&lt;/h3&gt;
&lt;p&gt;Few times faced slowness, although it is expected as there will be lot of people trying out Kiro along with Claude 4.0 Sonnet.
Claude 3.7 Sonnet felt bit more faster compared to 4.0 but considering the amount of people trying it out and being Kiro in preview phase, I can totally understand this.&lt;/p&gt;
&lt;h3 id=&#34;agent-hooks-1&#34;&gt;Agent Hooks&lt;/h3&gt;
&lt;p&gt;I might have caught a bug in Agent Hooks implementation or I might be wrong. Anyway, let me spell it out. Feel free to correct me if I understood it wrong.&lt;/p&gt;
&lt;p&gt;While trying out Agent hooks, I have configured an Agent hook to be triggered whenever a file is saved. This worked fine if I run one task at a time. Whenever a file save was done, Agent hook was added to the task queue and waited until the tasks before it are done.&lt;/p&gt;
&lt;p&gt;After running one task at a time, I thought I will click &amp;ldquo;Start Task&amp;rdquo; on all the tasks so that all tasks will be executed sequentially. Here, when the first task was being executed, my git commit Agent hook got triggered by a file save and added it to the queue at the end after all the tasks.&lt;/p&gt;
&lt;p&gt;I wanted the Agent hook to be executed after every task so that each task is saved in a separate git commit. However that did not work as I anticipated. This is my understanding of how Agent hook must be called.&lt;/p&gt;
&lt;p&gt;These are early days of the release of Kiro. Will find out more about this feature going forward.&lt;/p&gt;
&lt;h3 id=&#34;debugging-loop&#34;&gt;Debugging loop&lt;/h3&gt;
&lt;p&gt;Few times, when I asked it to debug and fix an issue, it fell into a circular debugging loop and it was doing the same changes over and over again for more than ten times. I had to cancel the task and ask it to explain what it was doing.&lt;/p&gt;
&lt;p&gt;This might not be an issue with Kiro per se but I think it is an issue with LLM.&lt;/p&gt;
&lt;h2 id=&#34;final-note&#34;&gt;Final note&lt;/h2&gt;
&lt;p&gt;Kiro, Agentic AI IDE from AWS has started on a good note and using Claude 4.0 Sonnet as their model choice might be their genius move.&lt;/p&gt;
&lt;p&gt;AI Coding IDEs have been the first multi-billion dollar usecase coming out of Generative AI. AWS move to introduce Kiro at this moment in time will surely capture sizable userbase considering you pay 19$ per month to get the industry top LLM for coding.&lt;/p&gt;
&lt;p&gt;If you have not tried Kiro, do give it a try. Let me know in comments what is your Kiro experience?&lt;/p&gt;
&lt;h2 id=&#34;follow-me&#34;&gt;Follow Me&lt;/h2&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Built an AI Agent using Strands Agents SDK</title>
      <link>https://vijay.eu/ai-lab/built-an-ai-agent/</link>
      <pubDate>Mon, 07 Jul 2025 23:44:09 +0300</pubDate>
      
      <guid>https://vijay.eu/ai-lab/built-an-ai-agent/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/strands.png&#34; alt=&#34;strands&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Built an AI Agent using Strands Agents SDK from Amazon Web Services (AWS) which calls my Kubernetes MCP server. Read more to find out…&lt;/p&gt;
&lt;h2 id=&#34;agents-and-mcp&#34;&gt;Agents and MCP&lt;/h2&gt;
&lt;p&gt;AI Agents and Model Context Protocol are the most popular concepts in Gen AI now. Now I have created an AI Agent which calls MCP server to debug issues in my K8s cluster.&lt;/p&gt;
&lt;p&gt;Recently I created a Model Context Protocol (MCP) server for &lt;a href=&#34;https://github.com/vijaykodam/kubernetes-readonly-mcp&#34;&gt;Kubernetes read-only operations&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;strands-agents-sdk&#34;&gt;Strands Agents SDK&lt;/h2&gt;
&lt;p&gt;AWS has created an SDK for building AI Agents called &lt;a href=&#34;https://strandsagents.com/latest/&#34;&gt;Strands Agents SDK&lt;/a&gt;. Now it added support for MCP as well.&lt;/p&gt;
&lt;p&gt;I used Strands Agents SDK and built an AI Agent which calls my K8s MCP server and debugs issues in my running K8s cluster.&lt;/p&gt;
&lt;p&gt;This demonstrates the AI Agents ability to achieve goals by perceiving the environment, reasoning and acting upon it using available tools like MCP servers.&lt;/p&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;
&lt;p&gt;Below is my detailed demo of my AI Agent. Links in comments about Strands and my MCP server.&lt;/p&gt;
&lt;iframe 
   src=&#34;https://www.youtube.com/embed/GDEtRssnpx4?si=7S3cvYg-2Bz5PtEa&#34; 
   width=&#34;560&#34; 
   height=&#34;315&#34; 
   title=&#34;Embedded Content&#34; 
   frameborder=&#34;0&#34; 
   allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
   referrerpolicy=&#34;strict-origin-when-cross-origin&#34;
   allowfullscreen&gt;
 &lt;/iframe&gt;

&lt;p&gt;What are you building with Strands Agents SDK?&lt;/p&gt;
&lt;h2 id=&#34;follow-me&#34;&gt;Follow Me&lt;/h2&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My thoughts on LLM Video Generation</title>
      <link>https://vijay.eu/ai-lab/llm-video-generation/</link>
      <pubDate>Thu, 26 Jun 2025 22:02:48 +0300</pubDate>
      
      <guid>https://vijay.eu/ai-lab/llm-video-generation/</guid>
      <description>&lt;p&gt;Generated this video using Google Veo3. I realized two things after generating this video &amp;hellip;&lt;/p&gt;
&lt;iframe 
  src=&#34;https://www.youtube.com/embed/PbsennA_HHo?si=SWpMcqVq1ovIjdcl&#34; 
  width=&#34;560&#34; 
  height=&#34;315&#34; 
  title=&#34;Embedded Content&#34; 
  frameborder=&#34;0&#34; 
  allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
  referrerpolicy=&#34;strict-origin-when-cross-origin&#34;
  allowfullscreen&gt;
&lt;/iframe&gt;

&lt;h2 id=&#34;prompt-engineering-is-still-the-king&#34;&gt;Prompt Engineering is still the KING.&lt;/h2&gt;
&lt;p&gt;As humans we know the world and we fill in the blanks.&lt;/p&gt;
&lt;p&gt;My prompt for this video was:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A white bread slice lying down on a table. An egg is besides it. Then its hands and legs pop out and it stands up. It picks up the egg. Walks on to the pan on a gas stove beside the table. The bread slice cracks the egg on the pan. Throws the egg shells away. Then jumps on the egg to become french toast.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is normal scenario for us but for a multi-modal LLM which needs frame-by-frame information, this is not enough information.&lt;/p&gt;
&lt;p&gt;After generating this video, I used ChatGPT to generate a detailed prompt for storyboarding and frame-by-frame detailed information on what happens in that frame. That video was good but it failed for a different reason. That brings me to my second realization.&lt;/p&gt;
&lt;h2 id=&#34;physics-is-hard-for-llms&#34;&gt;Physics is hard for LLMs&lt;/h2&gt;
&lt;p&gt;LLMs still struggle with the real-world physics. They are getting better but there is still some more improvement needed until they get really better. See how in the video a set of hands came to lift the bread slice first. Then another set of fingers appeared to crack open the eggs. At the end it didn&amp;rsquo;t become a french toast.&lt;/p&gt;
&lt;p&gt;Here is the second video I generated.&lt;/p&gt;
&lt;iframe 
  src=&#34;https://www.youtube.com/embed/hFX_Xi5m6gM?si=pcYZmjjeAekABcAM&#34; 
  width=&#34;560&#34; 
  height=&#34;315&#34; 
  title=&#34;Embedded Content&#34; 
  frameborder=&#34;0&#34; 
  allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
  referrerpolicy=&#34;strict-origin-when-cross-origin&#34;
  allowfullscreen&gt;
&lt;/iframe&gt;

&lt;p&gt;See how the bread slice cracks the egg and two butter knobs were thrown and the somehow egg white and yolk dropped from already cracked eggs shells. Also at the end, the bread slice was standing in the pan with two butter knobs. I should have prompted better.&lt;/p&gt;
&lt;p&gt;This is the detailed prompt for your reference.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DETAILED MULTI-MODAL VIDEO PROMPT  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; · GLOBAL SETTINGS  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;• Duration ≈ &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt; s &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;192&lt;/span&gt; frames&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;• Aspect Ratio / Resolution 16∶9, HD &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1280&lt;/span&gt; × &lt;span style=&#34;color:#ae81ff&#34;&gt;720&lt;/span&gt; px&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;• Visual Style Bright cartoon-realism; saturated colours; soft morning sunlight &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;key-light &lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt; ° camera-left&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;.  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;• Camera Single hand-held virtual camera, gentle dolly &amp;amp; pans that keep the bread character centred; f/2.8 depth-of-field.  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;• Audio Light kitchen ambience bed + spot SFX listed in the storyboard.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; · CHARACTER &amp;amp; PROP REFERENCES  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Asset       | Look &amp;amp; Texture                                                        | Size          | Notes |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;|-------------|-----------------------------------------------------------------------|---------------|-------|
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Bread slice | White sandwich bread, 1.5 cm thick, browned crust, cute eyes &amp;amp; mouth; flexible toasted-dough arms/legs ending in white cartoon gloves &amp;amp; red sneakers. | &lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt; × &lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt; cm | Idle breathing motion. |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Egg         | Large ivory hen’s egg with faint speckles; glossy.                    | &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; cm tall     | Weighty bounce. |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Pan         | &lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt; cm cast-iron skillet, seasoned black interior, subtle sheen.       | —             | Centered on burner; faint steam once heated. |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Gas stove   | Stainless cook-top, blue flame under pan &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;already lit&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;.               | —             | — |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Table       | Warm oak countertop with visible grain.                               | —             | &lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt; cm from stove. |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; · STORYBOARD &amp;amp; FRAME-BY-FRAME BREAKDOWN &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt; s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;192&lt;/span&gt; frames&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Timecode &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;s&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; | Camera / Composition                                   | Action &amp;amp; Animation                                               | Key SFX / FX                         | Validation Checks |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;|--------------|--------------------------------------------------------|------------------------------------------------------------------|--------------------------------------|-------------------|
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **0.0 – 0.5** | Static close-up; bread lying flat, egg &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; cm right.    | Subtle breathing in bread.                                       | Low room tone.                       | ✓ Both assets visible |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **0.5 – 1.0** | Same angle, slight dolly-in.                           | Four limbs pop out with squash-stretch; eyes blink awake.        | &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; quick “pop” whooshes.              | ✓ Limbs emerge, no overlap errors |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **1.0 – 1.3** | Medium shot; camera tilts up.                          | Bread stands, grabs egg with both hands.                         | Soft thud + cloth rustle.            | ✓ Egg securely held |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **1.3 – 3.0** | Side tracking shot following bread.                   | Bread walks &lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt; cm to stove &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;≈ &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt; steps at 0.19 s each&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;.           | Sneaker squeaks; faint egg rattle.   | ✓ No foot-slip; egg intact |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **3.0 – 3.5** | Over-shoulder high angle on pan.                       | Bread lifts egg overhead in anticipation.                        | Rising whoosh.                       | ✓ Pan &amp;amp; flame visible |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **3.5 – 4.0** | Two-frame insert on egg striking pan rim.              | Egg cracks; whites/yolk pour in; shells split cleanly.           | Sharp crack → sizzling ramp.         | ✓ No shell bits in yolk |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **4.0 – 4.3** | Return medium; bread flicks shells behind onto table.  | Shells arc back, land out of focus.                              | Light clink.                         | ✓ Disposal path clear |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **4.3 – 4.8** | Low stove-level angle.                                 | Bread crouches; anticipatory squash.                             | Sizzle continues.                    | ✓ Limb IK stable |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **4.8 – 5.3** | Slow-motion &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;0.5 s&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; mid-air front flip; camera up-tilt | Bread leaps &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt; cm, flips &lt;span style=&#34;color:#ae81ff&#34;&gt;180&lt;/span&gt; °, lands butter-side down on egg.   | “Boing” + louder sizzle on impact.   | ✓ Flip rotation accurate |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **5.3 – 6.5** | Top-down tight shot.                                   | Egg absorbs; bread surface turns golden-brown; steam rises.      | Crackling pops.                      | ✓ Colour shift to &lt;span style=&#34;color:#75715e&#34;&gt;#F6B66A |&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **6.5 – 7.0** | Pull-back hero shot, slight dolly-out.                 | Bread now French-toast, pats of butter &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; eyes; gives thumbs-up.| Cymbal “ta-da!”                      | ✓ Final pose held &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; frames |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| **7.0 – 8.0** | Freeze-frame hold with gentle vignette.                | Title card optional.                                             | Music sting trails off.              | ✓ All assets on screen |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; · CONTINUITY &amp;amp; TECHNICAL CHECKLIST  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1. **Assets present every frame:** bread/toast, pan, stove flame, table.  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2. **Lighting continuity:** single key + &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt; % fill; avoid popping shadows.  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3. **Physics:** gravity 9.8 m s⁻²; egg fluid viscosity; no mesh interpenetration.  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;4. **Animation rules:** limbs ≤ &lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt; % stretch; centre-of-mass forward during walk.  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;5. **Audio sync:** SFX start within ±2 frames of visuals.  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;6. **Per-frame metadata required:** camera transform, asset transforms &amp;amp; materials, light vectors, active particles, audio triggers.  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;7. **Export:** &lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt; fps PNG sequence &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;no alpha&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; + WAV &lt;span style=&#34;color:#ae81ff&#34;&gt;48&lt;/span&gt; kHz 24-bit stereo; master container ProRes &lt;span style=&#34;color:#ae81ff&#34;&gt;422&lt;/span&gt; HQ.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;END OF PROMPT — ready &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; multi-modal LLM ingestion.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What is your experience with LLM video generation? Did you try any other LLMs?&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Built an app using Lovable, vibecoding startup</title>
      <link>https://vijay.eu/ai-lab/lovable-app/</link>
      <pubDate>Wed, 25 Jun 2025 16:29:33 +0300</pubDate>
      
      <guid>https://vijay.eu/ai-lab/lovable-app/</guid>
      <description>&lt;p&gt;Built a game using &lt;a href=&#34;https://lovable.dev/&#34;&gt;Lovable&lt;/a&gt;, a vibe-coding AI platform, this weekend.&lt;/p&gt;
&lt;p&gt;Lovable organized AI Showdown during the weekend where users can build projects and compete using code-generating AI Models from OpenAI, Anthropic and Google.&lt;/p&gt;
&lt;p&gt;As part of my project, I have created a simple word ladder game, where the player can only change only single letter in each rung to add a valid English word. The game is to change the existing word one letter at a time into the target word.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/lovable.jpeg&#34; alt=&#34;snowdrop&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is the first time I am using Lovable and I am surprised by the maturity of the product. It is very intuitive to use and it creates complete working app in the first prompt itself.&lt;/p&gt;
&lt;p&gt;I spent 2 hours creating this game, mainly to improve the word list, figure out to build huge amount of valid 3, 4 and 5 letter words, test the functionality, change the UI, check for security issues, change the UX etc.&lt;/p&gt;
&lt;p&gt;Anton Osika and team has built an amazing product. Also happy that it is from the Nordics :)&lt;/p&gt;
&lt;p&gt;Feel free to try the game and give me your feedback.&lt;/p&gt;
&lt;p&gt;Game URL: &lt;a href=&#34;https://word-ladder.lovable.app/&#34;&gt;https://word-ladder.lovable.app/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploying LLMs on Amazon EKS using NVIDIA GPUs</title>
      <link>https://vijay.eu/ai-lab/deploying-llm-on-eks-and-nvidia-gpu/</link>
      <pubDate>Sun, 11 May 2025 19:43:44 +0300</pubDate>
      
      <guid>https://vijay.eu/ai-lab/deploying-llm-on-eks-and-nvidia-gpu/</guid>
      <description>&lt;p&gt;Today I have deployed an LLM inference solution on Amazon EKS using NVidia GPU.&lt;/p&gt;
&lt;p&gt;As part of my Generative AI hands-on learning, attended an AWS hands-on workshop, where I have deployed Mistral 7B Instruct v0.3 model using Ray Serve and vLLM on Amazon EKS.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Below is the architecture diagram of the LLM inference solution I deployed on EKS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/llm-on-eks.jpeg&#34; alt=&#34;llm-on-eks&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;components-used&#34;&gt;Components used&lt;/h2&gt;
&lt;p&gt;If you want to host your own models and control entire lifecycle for security or governance reasons then deploying LLM inference on Amazon EKS is a no-brainer.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/ray-overview/getting-started.html&#34;&gt;Ray&lt;/a&gt; is one of the popular open-source frameworks for building and managing generative AI applications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/serve/index.html&#34;&gt;Ray Serve&lt;/a&gt; is a scalable model serving library for building online inference APIs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.vllm.ai/en/stable/&#34;&gt;vLLM&lt;/a&gt; is a popular high-throughput and memory-efficient inference and serving engine for LLMs. vLLM supports Kubernetes.&lt;/p&gt;
&lt;p&gt;Used &lt;a href=&#34;https://github.com/ray-project/kuberay&#34;&gt;kuberay operator&lt;/a&gt; for deploying Ray. This operator handles all the complexity for you so I prefer this method for deploying Ray on K8s.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/ray-observability/getting-started.html&#34;&gt;Ray dashboard&lt;/a&gt; provides visibility into overall cluster health, jobs, nodes etc.&lt;/p&gt;
&lt;p&gt;Used &lt;a href=&#34;https://github.com/open-webui/open-webui&#34;&gt;Open WebUI&lt;/a&gt; for dashboard. Installed NVIDIA Data Center GPU Manager Exporter for monitoring NVIDIA GPU usage in Grafana.&lt;/p&gt;
&lt;p&gt;Currently, AFAIK, for getting monitoring data from NVIDIA GPUs you have to install the &lt;a href=&#34;https://github.com/NVIDIA/dcgm-exporter&#34;&gt;NVIDIA DCGM exporter&lt;/a&gt;.  It is straight-forward and exports needed metrics like GPU temperature, GPU Power usage, GPU utilization etc.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Ray, Open WebUI, vLLM, Mistral - All are open source software capable of scaling LLM inference very high. This is an exciting development for open source.&lt;/p&gt;
&lt;h2 id=&#34;follow-me&#34;&gt;Follow Me&lt;/h2&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, Generative AI, LLMs, MCP, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Capstone project for Google GenAI course</title>
      <link>https://vijay.eu/ai-lab/google-genai-capstone-project/</link>
      <pubDate>Sat, 19 Apr 2025 00:54:11 +0300</pubDate>
      
      <guid>https://vijay.eu/ai-lab/google-genai-capstone-project/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/veggie2.webp&#34; alt=&#34;veggie-recipe-generator&#34;&gt;
Image credit: Generated using GPT-4o.&lt;/p&gt;
&lt;h1 id=&#34;how-i-built-generative-ai-agent-for-veg-recipe-generator-with-langgraph-and-gemini&#34;&gt;How I built Generative AI Agent for Veg Recipe generator with LangGraph and Gemini&lt;/h1&gt;
&lt;h2 id=&#34;my-problem-with-finding-best-recipes&#34;&gt;My Problem with finding best recipes&lt;/h2&gt;
&lt;p&gt;Lately I have been spending lot of time browsing Internet or youtube cooking videos to find perfect recipe for cooking specific vegetables.
Sometimes I over boil broccoli and sometimes I spend time figuring out how to cook Mexican version of bell peppers dish.
During these times I wonder how great it would be to have a magic tool which could answer my queries directly.&lt;/p&gt;
&lt;h2 id=&#34;how-generative-ai-agent-saved-my-day&#34;&gt;How Generative AI agent saved my day?&lt;/h2&gt;
&lt;p&gt;Recently I took this hands-on &lt;strong&gt;“5-day Gen AI Intensive Course with Google”&lt;/strong&gt; and thought this might be a good way to solve my problem with recipes.&lt;/p&gt;
&lt;p&gt;This is the definition from Google’s whitepaper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“Generative AI agents are applications that strive to achieve goals by perceiving their environment and acting upon it using available tools. They extend the capabilities of standalone Generative AI models by integrating reasoning, logic, and access to external information.”&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;To put it simply: LLMs cannot interact with the external world or act  on the external events. GenAI agents use LLMs, reasoning, logic and external tools to achieve the objectives set by the user.&lt;/p&gt;
&lt;p&gt;This fits perfectly with my personal problem I mentioned about finding recipes. If I set the objective of generating recipes for a given vegetable and if I add tools to get recipes for such vegetables then the GenAI agent will reason about my request, use logic to find the recipes using perfect tool and format it in a human-readable format and present it to me.&lt;/p&gt;
&lt;p&gt;So, I decided to create a GenAI Agent for Veggie Recipe Generator which helps me with recipes for the vegetables I want. In this capstone project, I have used mock recipes to demonstrate what I have learnt in this course.&lt;/p&gt;
&lt;h3 id=&#34;demonstrates-three-genai-capabilities&#34;&gt;Demonstrates three GenAI capabilities&lt;/h3&gt;
&lt;p&gt;This project uses Google gemini-2.0-flash API and LangGraph.&lt;/p&gt;
&lt;p&gt;It will be demonstrating these GenAI capabilities using :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generative AI agents:&lt;/strong&gt; Using LangGraph to create multi-step conversational flow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Structured output:&lt;/strong&gt; Making the LLM return information in a predictable human-readable format.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Few-shot prompting:&lt;/strong&gt; Guiding LLM’s behavior with clear examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-solution&#34;&gt;The Solution&lt;/h2&gt;
&lt;p&gt;This solution uses an &lt;em&gt;agent&lt;/em&gt; built with LangGraph and powered by Google Gemini API. It follows defined graph to manage the conversation.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It greets the user.&lt;/li&gt;
&lt;li&gt;It understands the user&amp;rsquo;s request for a vegetable recipe.&lt;/li&gt;
&lt;li&gt;It checks if the vegetable is one it knows (Carrot, Broccoli, etc.).&lt;/li&gt;
&lt;li&gt;If valid, it uses a &amp;ldquo;tool&amp;rdquo; (a simulated function) to look up the recipe.&lt;/li&gt;
&lt;li&gt;If invalid, it politely informs the user of its limitations.&lt;/li&gt;
&lt;li&gt;It presents the recipe (if found) in a clear, structured format.&lt;/li&gt;
&lt;li&gt;It asks the user if they want another recipe or want to quit.&lt;/li&gt;
&lt;li&gt;It loops back or exits based on the user&amp;rsquo;s response.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;the-code&#34;&gt;The Code&lt;/h2&gt;
&lt;h3 id=&#34;state-management&#34;&gt;State Management&lt;/h3&gt;
&lt;p&gt;LangGraph uses state dictionary to pass between nodes. Here is it’s structure.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; typing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Annotated, List, Literal, TypedDict &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; TypeHint
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; typing_extensions &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; TypedDict
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langgraph.graph.message &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; add_messages
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_core.messages &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; BaseMessage
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;### Define the structure for the recipe data returned by the tool&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Recipe&lt;/span&gt;(TypeHint):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    title: str
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ingredients: List[str]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    instructions: str
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;RecipeState&lt;/span&gt;(TypedDict):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;State representing the user&amp;#39;s recipe conversation.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    messages: Annotated[List[BaseMessage], add_messages] &lt;span style=&#34;color:#75715e&#34;&gt;# Conversation history&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    finished: bool &lt;span style=&#34;color:#75715e&#34;&gt;# Flag to end the conversation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;guiding-llm-using-few-shot-prompting-and-structured-output&#34;&gt;Guiding LLM using few-shot prompting and structured output&lt;/h3&gt;
&lt;p&gt;The core instructions tell the AI its role, limitations, &lt;em&gt;how&lt;/em&gt; to use tools, and crucially, &lt;em&gt;how to format the structured output&lt;/em&gt; it receives from the tool. We include examples (few-shot prompting) to make these instructions clearer:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Snippet from RECIPEBOT_SYSINT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;RECIPEBOT_SYSINT &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You are RecipeBot... You can provide recipes ONLY for... Carrot, Broccoli, Spinach, Potato, Bell Pepper.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;When the user asks for a recipe... you MUST use the &amp;#39;get_recipe&amp;#39; tool...&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The &amp;#39;get_recipe&amp;#39; tool will return structured information with &amp;#39;title&amp;#39;, &amp;#39;ingredients&amp;#39;, and &amp;#39;instructions&amp;#39;.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;When you receive the recipe from the tool, present it clearly to the user in this format:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Okay, here is a recipe for [Vegetable Name]:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;**[Recipe Title]**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ingredients:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n- [Ingredient 1]&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n- [Ingredient 2]&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n...&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Instructions:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n [Instructions text]&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;If the user asks for a vegetable not on the list ..., politely tell them you only have recipes for those listed vegetables, and ask if they want one of those instead.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;After providing a recipe... ALWAYS ask the user if they would like another recipe... or if they want to quit.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Example Interactions:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Human: Can I get a recipe for carrots?&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;AI: &amp;lt;tool_call&amp;gt;get_recipe(vegetable=&amp;#39;carrot&amp;#39;)&amp;lt;/tool_call&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Tool: {&amp;#39;title&amp;#39;: &amp;#39;Simple Roasted Carrots&amp;#39;, &amp;#39;ingredients&amp;#39;: [&amp;#39;1lb chopped carrots&amp;#39;, ...], &amp;#39;instructions&amp;#39;: &amp;#39;Toss chopped carrots...&amp;#39;}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;AI: Okay, here is a recipe for Carrot:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n**Simple Roasted Carrots**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nIngredients:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n- 1lb chopped carrots&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n...&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nInstructions:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nToss chopped carrots...&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nWould you like another recipe... or would you like to quit?&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Human: Do you have one for Zucchini?&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;AI: I&amp;#39;m sorry, I only have recipes for Carrot, Broccoli, Spinach, Potato, and Bell Pepper. Would you like one for those?&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# ... more examples ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;get_recipe-tool&#34;&gt;get_recipe tool&lt;/h3&gt;
&lt;p&gt;We define a Python function decorated with LangChain&amp;rsquo;s &lt;code&gt;@tool&lt;/code&gt;. This function simulates looking up a recipe and returns it as a structured Python dictionary (&lt;code&gt;Recipe&lt;/code&gt;), not just a block of text.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_core.tools &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tool
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; typing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Dict, Any, Optional
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Mock Recipe Data (Structured)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;MOCK_RECIPES: Dict[str, Recipe] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;carrot&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;title&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Simple Roasted Carrots&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ingredients&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;instructions&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;...&amp;#34;&lt;/span&gt;},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;broccoli&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;title&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Steamed Broccoli with Lemon&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ingredients&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;instructions&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;...&amp;#34;&lt;/span&gt;},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# ... other recipes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@tool&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_recipe&lt;/span&gt;(vegetable: str) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; Optional[Recipe]: &lt;span style=&#34;color:#75715e&#34;&gt;# Returns structured Recipe or None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Provides a simple structured vegetarian recipe (title, ingredients, instructions) for the specified vegetable. Only works for Carrot, Broccoli, Spinach, Potato, Bell Pepper.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    recipe_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MOCK_RECIPES&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(vegetable&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; recipe_data:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; recipe_data
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Returning &lt;em&gt;structured output&lt;/em&gt; from the tool allows the chatbot node to reliably extract and format the title, ingredients, and instructions according to the system prompt&amp;rsquo;s rules.
Currently we are returning hardcoded mock data.&lt;/p&gt;
&lt;h3 id=&#34;building-the-graph-with-langgraph&#34;&gt;Building the graph with LangGraph&lt;/h3&gt;
&lt;p&gt;We combine the nodes which perform actions, edges which define flow between the nodes and conditional logic to when to call a tool? or When to quit?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langgraph.graph &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; StateGraph, START, END
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# --- Build Graph ---&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print(&amp;#34;Building graph...&amp;#34;) # Debug&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;graph_builder &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; StateGraph(RecipeState)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Add nodes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;graph_builder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_node(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chatbot&amp;#34;&lt;/span&gt;, chatbot_node)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;graph_builder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_node(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;human&amp;#34;&lt;/span&gt;, human_node)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;graph_builder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_node(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tools&amp;#34;&lt;/span&gt;, tool_node)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define entry point&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;graph_builder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_edge(START, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chatbot&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define edges with conditional routing&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;graph_builder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_conditional_edges(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chatbot&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    route_logic,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tools&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tools&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;human&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;human&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;graph_builder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_conditional_edges(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;human&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    exit_logic,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chatbot&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chatbot&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        END: END
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Tool node always returns to chatbot&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;graph_builder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_edge(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tools&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chatbot&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Compile the graph&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;recipe_graph &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; graph_builder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;kaggle-notebook&#34;&gt;Kaggle notebook&lt;/h2&gt;
&lt;p&gt;If you want to see the code or would like to run it, you can find the jupyter notebook on &lt;a href=&#34;https://www.kaggle.com/code/vijaykodam/gen-ai-agent-for-veg-recipe-generator&#34;&gt;Kaggle here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Feel free to give me your feedback!&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Currently get_recipes tool uses MOCK_RECIPES dictionary hardcoded. In the future, it would be great to search the Internet or use RAG to search from an embedding vector database.&lt;/li&gt;
&lt;li&gt;Limited it to only 5 vegetables to keep it simple. It could be expanded to include list of ingredients and also allergies and some ingredients to avoid.&lt;/li&gt;
&lt;li&gt;Current chat box is very basic. It would be great to add voice input.&lt;/li&gt;
&lt;li&gt;Add more tools to find supermarkets to buy the ingredients.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This capstone project demonstrates how combining the conversational power of Google&amp;rsquo;s Gemini model with the structured workflow capabilities of LangGraph allows for the creation of useful and interactive AI agents. By leveraging &lt;strong&gt;few-shot prompting&lt;/strong&gt; and handling &lt;strong&gt;structured tool outputs&lt;/strong&gt; within an &lt;strong&gt;agentic framework&lt;/strong&gt;, I have solved my recipe issue. There is a lot more one can build with Generative AI Agents.&lt;/p&gt;
&lt;h3 id=&#34;follow-me&#34;&gt;Follow me&lt;/h3&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating decibel meter using Claude Code Agentic tool</title>
      <link>https://vijay.eu/ai-lab/decibel-meter-using-claude-code/</link>
      <pubDate>Thu, 13 Mar 2025 23:16:20 +0200</pubDate>
      
      <guid>https://vijay.eu/ai-lab/decibel-meter-using-claude-code/</guid>
      <description>&lt;h2 id=&#34;claude-code&#34;&gt;Claude Code&lt;/h2&gt;
&lt;p&gt;Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster through natural language commands.&lt;/p&gt;
&lt;p&gt;For the last few weeks, I have been experimenting with Claude Code. The more I use, the more I am in awe of this product. It is not perfect but almost there. Works very nicely with small codebase.&lt;/p&gt;
&lt;h2 id=&#34;idea&#34;&gt;Idea&lt;/h2&gt;
&lt;p&gt;Yesterday, I wanted to build a decibel meter app for my kids to know about decibels, how sound is measured and just to give them a new toy to play with. For me, I was already playing with Claude code, the agentic coding toy :)&lt;/p&gt;
&lt;h3 id=&#34;requirements&#34;&gt;Requirements&lt;/h3&gt;
&lt;p&gt;I wanted a simple browser-based decibel meter that uses my device&amp;rsquo;s microphone to measure sound levels.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;App must use the Web Audio API to access microphone data.&lt;/li&gt;
&lt;li&gt;I am okay with approximate decibel measurements as this will not be calibrated.&lt;/li&gt;
&lt;li&gt;No data must be stored or transmitted - all processing must happen locally in my browser.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;my-app-building-process&#34;&gt;My App building process&lt;/h3&gt;
&lt;p&gt;Went into my codebase for my blog, opened Claude Code and started prompting to it. After a few revisions the first version was ready in 30 mins and $1.34. It added 560 lines and removed 169 lines in total.&lt;/p&gt;
&lt;p&gt;I have reviewed the code, tested the functionality. Yesterday I learnt that browsers do not allow microphone access to localhost or HTTP websites. Only HTTPS websites are allowed. I had to copy paste my code into Codepen to test it before pushing into production. After verifying, I had published my project.&lt;/p&gt;
&lt;h3 id=&#34;cost&#34;&gt;Cost&lt;/h3&gt;
&lt;p&gt;I like the handy &lt;code&gt;/cost&lt;/code&gt; command to get the amount I burned on this toy project for that session.
You also get the final costs when you exit the claude code.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; /cost
  ⎿  Total cost:            $1.34
     Total duration (API):  5m 5.9s
     Total duration (wall): 36m 57.8s
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;version-1&#34;&gt;Version 1&lt;/h3&gt;
&lt;p&gt;Here is my first version UI.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/decibel-v11.webp&#34; alt=&#34;first-version&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;real-world-feedback&#34;&gt;Real World Feedback&lt;/h3&gt;
&lt;p&gt;When I tested it in my mobile, I realized that the sensitivity for normal voice was very low and the decibel meter was not moving at all. This was not a usable app, if the decibel meter barely moved.&lt;/p&gt;
&lt;p&gt;Now was the time for version 2 of the decibel meter app. With the real world testing I had new set of requirements to make it any useful. Also the decibel meter was showing very tiny fraction for the normal voice range.&lt;/p&gt;
&lt;h2 id=&#34;version-2&#34;&gt;Version 2&lt;/h2&gt;
&lt;p&gt;Now for the version 2, based on my testing, real world feedback had a new set of requirements.
The long vertical decibel meter was not usable. So the next way to represent sound was to use a speedometer.
Also the sensitivity should be high so that there is more spectrum for normal voice range and lesser for loud or extremely loud sounds. If there was an option to set microphone sensitivity, then it would be even better.&lt;/p&gt;
&lt;p&gt;The version 2 took several iterations as old code must be removed and new functionality added. Spent nearly 50 minutes to get it working.&lt;/p&gt;
&lt;h3 id=&#34;continuous-feedback&#34;&gt;Continuous feedback&lt;/h3&gt;
&lt;p&gt;While using Claude code, realized that I was practically testing the app UI after every iteration of code change, checking for errors in console logs, and prompting it to fix those errors.&lt;/p&gt;
&lt;h3 id=&#34;gotchas&#34;&gt;Gotchas&lt;/h3&gt;
&lt;p&gt;One time, I didn&amp;rsquo;t realize the app was using old version of javascript and I thought the app was not working.  Claude Code was trying to fix a non-existent error, happily following my instructions and generating or modifying code continuously. After opening local version in a private window to get rid of cache and cookies, it started working. This could have saved me 15 mins if I had noticed earlier.&lt;/p&gt;
&lt;p&gt;So be careful to verify and test properly. Currently, Claude code will generate or modify code if you say it is not working.&lt;/p&gt;
&lt;h3 id=&#34;the-final-ui&#34;&gt;The Final UI&lt;/h3&gt;
&lt;p&gt;After several iterations and centering the needle, fixing the needle size, colors, shape of speedometer and 50 minutes of my time, got below UI as version 2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/decibel-v2.webp&#34; alt=&#34;version2&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;decibel-meter-project&#34;&gt;Decibel Meter project&lt;/h2&gt;
&lt;p&gt;Here is my &lt;a href=&#34;https://vijay.eu/projects/decibel-meter/&#34;&gt;Decibel meter project&lt;/a&gt; if you want to try and check it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note that it is not been calibrated and the values are approximate.&lt;/li&gt;
&lt;li&gt;No data is stored or transmitted - all processing happens locally in your browser&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;version-2-cost&#34;&gt;Version 2 Cost&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;V2 cost
Total cost:            $2.32
Total duration (API):  12m 14.8s
Total duration (wall): 50m 10.9s
Total code changes:    1373 lines added, 501 lines removed
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;other-projects&#34;&gt;Other Projects&lt;/h3&gt;
&lt;p&gt;I prefer building apps to solve my problems or create apps for my kids to play or learn. These days I am only using Claude Code agentic coding tool. I like working in terminal and Claude code runs entirely in terminal, no IDE, which works perfectly for me.&lt;/p&gt;
&lt;p&gt;I have built Cookie consent functionality for my blog. Also built &lt;a href=&#34;https://vijay.eu/projects/markdown-editor/&#34;&gt;Markdown Editor&lt;/a&gt; to help me while writing my blogs in markdown and &lt;a href=&#34;https://vijay.eu/projects/snake-game/&#34;&gt;Classic Snake Game&lt;/a&gt;  for my kids to play.&lt;/p&gt;
&lt;p&gt;All these three apps have been built using Claude Code and spent less than an hour to get it published. End-to-end, from ideation to running in the web and using it in my Mobile/Mac.&lt;/p&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;These days, I started coding or should I say prompting a lot and generating lot of code using Claude Code than before.
I am constantly on the lookout for the next app idea to implement. Building feels great. Seeing end-to-end working solutions feels great.&lt;/p&gt;
&lt;p&gt;Claude code is not cheap. It is addictive as you literally have a AI coder at your disposal.&lt;/p&gt;
&lt;h3 id=&#34;your-experience&#34;&gt;Your experience&lt;/h3&gt;
&lt;p&gt;How was your experience with Claude Code? Have you used any other agentic coding tools?&lt;/p&gt;
&lt;h2 id=&#34;follow-me&#34;&gt;Follow me&lt;/h2&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Claude Code Agentic CLI demo</title>
      <link>https://vijay.eu/ai-lab/claude-code-demo/</link>
      <pubDate>Mon, 24 Feb 2025 23:58:17 +0200</pubDate>
      
      <guid>https://vijay.eu/ai-lab/claude-code-demo/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/claude-code-shell.png&#34; alt=&#34;claude-code&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here is my Day Zero hands-on demo of Claude Code using Claude 3.7 Sonnet. Both were released by Anthropic today.&lt;/p&gt;
&lt;p&gt;Claude 3.7 Sonnet, is the first hybrid reasoning model on the market by Anthropic. They have also introduced a command line tool for agentic coding, Claude Code.&lt;/p&gt;
&lt;p&gt;I have installed in my mac and tried agentic code and made a demo video for you.&lt;/p&gt;
&lt;iframe 
  src=&#34;https://www.youtube.com/embed/mFN-MGEVMMI?si=a_ZpbzvN6GBuacZu&#34; 
  width=&#34;560&#34; 
  height=&#34;315&#34; 
  title=&#34;Embedded Content&#34; 
  frameborder=&#34;0&#34; 
  allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
  referrerpolicy=&#34;strict-origin-when-cross-origin&#34;
  allowfullscreen&gt;
&lt;/iframe&gt;

&lt;p&gt;Take a look and add in comments how are you planning to use this shiny new agentic code tool in your workflow?&lt;/p&gt;
&lt;p&gt;It cost me $0.3273 for 1 minute usage of Anthropic API. It is costly for personal use. That is my personal feeling.
&lt;img src=&#34;https://vijay.eu/images/claude-code-cost.png&#34; alt=&#34;claude-code-cost&#34;&gt;&lt;/p&gt;
&lt;p&gt;I would rather want this Claude code like agentic code tool to run locally and call a local llm running on my mac. Everything local, and secure solution. Of course, there will be some opensource tool out in the market in a month doing the same thing. Let&amp;rsquo;s hope so&amp;hellip;&lt;/p&gt;
&lt;p&gt;What do you think? Write in comments.&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classic Snake Game using LLMs</title>
      <link>https://vijay.eu/ai-lab/snake-game/</link>
      <pubDate>Sun, 16 Feb 2025 23:23:47 +0200</pubDate>
      
      <guid>https://vijay.eu/ai-lab/snake-game/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/SnakeRedApple.jpeg&#34; alt=&#34;snake&#34;&gt;
Image Credit: Generated using Google Imagen3.&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;Last August, I have spent a week trying to create working applications using GenAI models. These were meant to be useful for me and my kids and to assess the capabilities of the then popular LLMs like ChatGPT, Claude, and Gemini. You can find them here &lt;a href=&#34;https://vijay.eu/projects/&#34;&gt;https://vijay.eu/projects/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;my-initial-experience&#34;&gt;My initial experience&lt;/h2&gt;
&lt;p&gt;This was my experience back then: Ask each LLM to generate code for a project, copy the code into local files, test it locally, come up with new suggestions and then rinse and repeat until I found the project working well as per my expectation.&lt;/p&gt;
&lt;p&gt;For this exercise I have used: Anthropic Claude, Google Gemini, and OpenAI ChatGPT.
Each had a different way of generating code, handling code changes on top of existing code and updating code.&lt;/p&gt;
&lt;p&gt;From my experience at that time, Anthropic Claude understood prompts really well and generated code much better than all other LLMs. The quality of code was way better than others. One major advantage was Claude Artifacts. This was a deal breaker for me and game changer for testing quick prototypes in the chat window itself. Also Claude allowed you to host the code and gave you an URL to share with your friends. Getting the html files and testing locally also worked seemlessly. It let you download the files. No copy pasting code from the chat window, which was the case for the remaining two LLMs.&lt;/p&gt;
&lt;h2 id=&#34;new-challenge&#34;&gt;New Challenge&lt;/h2&gt;
&lt;p&gt;Then came the other challenge. My website(&lt;a href=&#34;https://vijay.eu&#34;&gt;https://vijay.eu&lt;/a&gt;) is based on Hugo and it is a static website hosted on GitHub Pages. GitHub delivers the static webpages to your mobile/computer when you open my website. There is no dynamic content in my website so there is no need of backend server to process the requests.&lt;/p&gt;
&lt;p&gt;I wanted to host my projects created using LLMs also on my website. This means, those projects should be fully static and should not require a backend server to handle any requests for the frontend running in user&amp;rsquo;s mobile or computer.&lt;/p&gt;
&lt;p&gt;Hugo uses templating which simplifies adding new posts, which means all posts have the same header, footer, common javascript and common CSS. However for my projects, most of the magic is handled as part of Javascript and CSS.&lt;/p&gt;
&lt;p&gt;I spent several hours tweaking the code, explaining this to LLMs and asking them to generate the code which works with my existing hugo code. This took quite an effort and took several trial and errors before the first project worked. When I wanted to create a second flag game I had to do all this over again as it included a different design and there were upto ten questions each coming one after another. I enjoyed building those games but thought these should have been handled automatically for me.&lt;/p&gt;
&lt;h2 id=&#34;current-experience-with-llms&#34;&gt;Current experience with LLMs&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Today after four months&lt;/strong&gt;, I wanted to check how far these LLMs have improved. So I decided to create classic snake game for my kids to play. Asked the same prompt to Claude, Gemini and ChatGPT. ChatGPT has new feature similar to Claude&amp;rsquo;s artifact. ChatGPT created the code and displayed the code and demo window. When I pressed Preview, it kinda worked until the green snake (a single green square) ate the first red square. After that it all went haywire. There were multiple red squares appearing randomly across the box and the green square does not move.&lt;/p&gt;
&lt;p&gt;Gemini generated code in a window and no option to run locally in the window. Copied the code to my computer and ran it locally. It didn&amp;rsquo;t work. There was a black box and nothing else. No red or green squares.&lt;/p&gt;
&lt;p&gt;Now, Claude generated perfectly working code, tested it in the artifact window and also customized the code to work with my hugo website. It also gave me instructions to add shortcodes in my hugo to add new javascript and css for this game. I felt the snake was moving too fast and asked Claude to generate new version to add three different speeds. It did on first try. I have tested it locally then added it my blog and pushed the changes. You can try this snake game on my website at &lt;a href=&#34;https://vijay.eu/projects/snake-game/&#34;&gt;https://vijay.eu/projects/snake-game/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;winner-this-time&#34;&gt;Winner this time&lt;/h3&gt;
&lt;p&gt;So, in this second round of my game benchmarking of LLMs, the winner is &lt;strong&gt;Anthropic Claude&lt;/strong&gt; :)&lt;/p&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;These LLMs are now getting more advanced, producing error free code and understanding user prompts much better. This took me 10 minutes to prompt, test, download and push it to my website. TEN minutes from getting the idea to publishing it live on my website. This is A-W-E-S-O-M-E. You just have to imagine and you have unlimited digital workers, a.k.a LLMs, realizing those dreams for you. Democratizing code for everyone. This gives more power to people who can create end-to-end projects from idea to design to code to deploying it in real world production.&lt;/p&gt;
&lt;p&gt;How was your experience working with LLMs? What difference have you observed over the last six months?&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hands-on Bedrock Agents workshop</title>
      <link>https://vijay.eu/ai-lab/bedrock-agents-workshop/</link>
      <pubDate>Sat, 15 Feb 2025 00:22:33 +0200</pubDate>
      
      <guid>https://vijay.eu/ai-lab/bedrock-agents-workshop/</guid>
      <description>&lt;p&gt;Are agents the missing piece for LLMs to super-charge real-world adoption? Today, I had the opportunity to attend AWS Immersion Day at AWS Helsinki office and try hands-on workshop on Agents, RAG, and more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/bedrock-ws-1.webp&#34; alt=&#34;bedrock-workshop&#34;&gt;&lt;/p&gt;
&lt;p&gt;Thanks Amazon Web Services (AWS), Ankit Nigam, Kimmo Isosomppi, Heidi Tähtinen, Ezaldeen Arafat for organizing this workshop. This was immensely useful and improved my understanding of Agents, RAG, Guardrails and inference.&lt;/p&gt;
&lt;p&gt;As part of the hands-on workshop, I have created a mortage-processing-agent which uses RAG to add domain specific knowledge base and then created agents which can determine if end-user can afford it based on their financial situation.
After that, created Amazon Bedrock Flows that orchestrates entire workflow. Last but not the least, created Guardrails to implement safeguards and enforce responsible AI policies for my GenAI application.&lt;/p&gt;
&lt;p&gt;Here is the mortgage processing agent I built during the hands-on workshop.
The highlighted yellow box is the prompt user enters followed by the response from Bedrock.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/bedrock-ws-handson.webp&#34; alt=&#34;bedrock-handson&#34;&gt;&lt;/p&gt;
&lt;p&gt;All in all, this was a very good way to get hands-on experience on Amazon Bedrock Agents, RAG, Flows and Guardrails.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/bedrock-ws-agents.webp&#34; alt=&#34;bedrock-agents&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running Deepseek R1 locally</title>
      <link>https://vijay.eu/ai-lab/running-deepseek-locally/</link>
      <pubDate>Wed, 29 Jan 2025 22:12:06 +0200</pubDate>
      
      <guid>https://vijay.eu/ai-lab/running-deepseek-locally/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://vijay.eu/images/snowdrop.webp&#34; alt=&#34;snowdrop&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here is a video of Deepseek R1 running locally on my Macbook.
Now that everyone is amazed by the low level of resource utilization and open weight model of Deepseek R1, installed 8B model locally.&lt;/p&gt;
&lt;p&gt;With the help of ollama it is as simple as running a single command -
&amp;ldquo;ollama run deepseek-r1:8b&amp;rdquo;
You get your own private, local LLM running securely in your computer.&lt;/p&gt;
&lt;p&gt;My prompt is &amp;ldquo;Generate a five line fairy tale about AI?&amp;rdquo;. In the video, you can see how before generating the response, it thinks. Thinking text is in between &lt;think&gt; and &lt;/think&gt; tags.&lt;/p&gt;
&lt;p&gt;Watch the video to read the five line fairy tale about AI.&lt;/p&gt;
&lt;p&gt;Do you have any experience running Deepseek R1?&lt;/p&gt;
&lt;iframe 
  src=&#34;https://www.youtube.com/embed/CYfpgvsTG9E?si=QKfJcJB1vjJIRWwe&#34; 
  width=&#34;560&#34; 
  height=&#34;315&#34; 
  title=&#34;Embedded Content&#34; 
  frameborder=&#34;0&#34; 
  allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
  referrerpolicy=&#34;strict-origin-when-cross-origin&#34;
  allowfullscreen&gt;
&lt;/iframe&gt;

&lt;p&gt;If you are new to my posts, I regularly post about AWS, EKS, Kubernetes and Cloud computing related topics. Do follow me in &lt;a href=&#34;https://www.linkedin.com/in/vijaykodam/&#34;&gt;LinkedIn&lt;/a&gt; and visit &lt;a href=&#34;https://dev.to/vijaykodam&#34;&gt;my dev.to posts&lt;/a&gt;. You can find all my previous blog posts in &lt;a href=&#34;https://vijay.eu/posts&#34;&gt;my blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
